diff --git a/CODEOWNERS b/CODEOWNERS
index 7007e80b2..74caea2c4 100644
--- a/CODEOWNERS
+++ b/CODEOWNERS
@@ -42,8 +42,8 @@
 /plugin/          @pllarroy
 
 # CMake
-CMakeLists.txt    @szha @rahul003 @pllarroy
-/cmake/           @szha @rahul003 @pllarroy
+CMakeLists.txt    @szha @pllarroy
+/cmake/           @szha @pllarroy
 
 # MXNet CI
 dev_menu.py         @pllarroy
@@ -71,4 +71,3 @@ prepare_mkl.sh    @szha
 
 # Github templates
 /.github/         @szha
-
diff --git a/Makefile b/Makefile
index 439e612ad..c6ca8f053 100644
--- a/Makefile
+++ b/Makefile
@@ -162,7 +162,7 @@ ifdef CAFFE_PATH
 endif
 
 ifndef LINT_LANG
-	LINT_LANG="all"
+	LINT_LANG = "all"
 endif
 
 ifeq ($(USE_MKLDNN), 1)
@@ -210,6 +210,7 @@ endif
 
 ifeq ($(USE_OPENMP), 1)
 	CFLAGS += -fopenmp
+	CFLAGS += -DMXNET_USE_OPENMP=1
 endif
 
 ifeq ($(USE_NNPACK), 1)
@@ -272,7 +273,7 @@ ifeq ($(USE_CUDNN), 1)
 	LDFLAGS += -L/opt/rocm/miopen/lib -lMIOpen #-lcudnn
 endif
 
-ifeq ($(USE_BLAS), open)
+ifeq ($(USE_BLAS), openblas)
 	CFLAGS += -DMXNET_USE_BLAS_OPEN=1
 else ifeq ($(USE_BLAS), atlas)
 	CFLAGS += -DMXNET_USE_BLAS_ATLAS=1
@@ -396,6 +397,12 @@ endif
 
 # Guard against displaying nvcc info messages to users not using CUDA.
 ifeq ($(USE_GPU), 1)
+	# Get AR version, compare with expected ar version and find bigger and smaller version of the two
+	AR_VERSION := $(shell ar --version | egrep -o "([0-9]{1,}\.)+[0-9]{1,}")
+	EXPECTED_AR_VERSION := $(shell echo "2.27")
+	LARGE_VERSION := $(shell printf '%s\n' "$(AR_VERSION)" "$(EXPECTED_AR_VERSION)" | sort -V | tail -n 1)
+	SMALL_VERSION := $(shell printf '%s\n' "$(AR_VERSION)" "$(EXPECTED_AR_VERSION)" | sort -V | head -n 1)
+
 	# If NVCC is not at the location specified, use CUDA_PATH instead.
 	ifeq ("$(wildcard $(NVCC))","")
 		ifneq ($(USE_CUDA_PATH), NONE)
@@ -593,7 +600,7 @@ build/plugin/%.o: plugin/%.cc
 
 %_gpu.o: %.cu
 	@mkdir -p $(@D)
-	$(NVCC) $(NVCCFLAGS) $(CUDA_ARCH) -Xcompiler "$(HIPCCFLAGS) -Isrc/operator" -M -MT $*_gpu.o $< >$*_gpu.d
+	$(NVCC) $(NVCCFLAGS) $(CUDA_ARCH) -Xcompiler "$(HIPCFLAGS) -Isrc/operator" --generate-dependencies -MT $*_gpu.o $< >$*_gpu.d
 	$(NVCC) -c -o $@ $(NVCCFLAGS) $(CUDA_ARCH) -Xcompiler "$(HIPCFLAGS) -Isrc/operator" $<
 
 %.o: %.cc $(CORE_INC)
@@ -689,7 +696,7 @@ lint: cpplint rcpplint jnilint pylint
 
 cpplint:
 	3rdparty/dmlc-core/scripts/lint.py mxnet cpp include src plugin cpp-package tests \
-	--exclude_path src/operator/contrib/ctc_include
+	--exclude_path src/operator/contrib/ctc_include include/mkldnn
 
 pylint:
 	python3 -m pylint --rcfile=$(ROOTDIR)/ci/other/pylintrc --ignore-patterns=".*\.so$$,.*\.dll$$,.*\.dylib$$" python/mxnet tools/caffe_converter/*.py
@@ -772,15 +779,15 @@ rclean:
 	$(RM) -r R-package/src/image_recordio.h R-package/NAMESPACE R-package/man R-package/R/mxnet_generated.R \
 		R-package/inst R-package/src/*.o R-package/src/*.so mxnet_*.tar.gz
 
-build/rat/apache-rat/target/apache-rat-0.13-SNAPSHOT.jar:
+build/rat/apache-rat/target/apache-rat-0.13.jar:
 	mkdir -p build
-	svn co http://svn.apache.org/repos/asf/creadur/rat/branches/0.12-release/ build/rat; \
+	svn co http://svn.apache.org/repos/asf/creadur/rat/tags/apache-rat-project-0.13/ build/rat; \
 	cd build/rat; \
 	mvn -Dmaven.test.skip=true install;
 
-ratcheck: build/rat/apache-rat/target/apache-rat-0.13-SNAPSHOT.jar
+ratcheck: build/rat/apache-rat/target/apache-rat-0.13.jar
 	exec 5>&1; \
-	RAT_JAR=build/rat/apache-rat/target/apache-rat-0.13-SNAPSHOT.jar; \
+	RAT_JAR=build/rat/apache-rat/target/apache-rat-0.13.jar; \
 	OUTPUT=$(java -jar $(RAT_JAR) -E tests/nightly/apache_rat_license_check/rat-excludes -d .|tee >(cat - >&5)); \
     ERROR_MESSAGE="Printing headers for text files without a valid license header"; \
     echo "-------Process The Output-------"; \
diff --git a/benchmark/opperf/README.md b/benchmark/opperf/README.md
index e6a367d38..7e708d3cb 100644
--- a/benchmark/opperf/README.md
+++ b/benchmark/opperf/README.md
@@ -75,7 +75,7 @@ For example, you want to run benchmarks for all NDArray Broadcast Binary Operato
 
 ```
 #!/usr/bin/python
-from benchmark.opperf.tensor_operations.binary_broadcast_operators import run_mx_binary_broadcast_operators_benchmarks
+from benchmark.opperf.nd_operations.binary_broadcast_operators import run_mx_binary_broadcast_operators_benchmarks
 
 # Run all Binary Broadcast operations benchmarks with default input values
 print(run_mx_binary_broadcast_operators_benchmarks())
diff --git a/benchmark/opperf/nd_operations/README.md b/benchmark/opperf/nd_operations/README.md
index 321158c48..95958662a 100644
--- a/benchmark/opperf/nd_operations/README.md
+++ b/benchmark/opperf/nd_operations/README.md
@@ -28,9 +28,7 @@
 6. reshape
 7. one_hot
 8. linalg_potri
-9. mp_sgd_update
 10. multi_sgd_update
-11. signum_update
 12. Convolution_v1
 13. repeat
 14. Custom
@@ -38,7 +36,6 @@
 16. SwapAxis
 17. norm
 18. Softmax
-19. rmspropalex_update
 20. fill_element_0index
 21. cast
 22. UpSampling
@@ -52,7 +49,6 @@
 30. Activation
 31. LinearRegressionOutput
 32. Pooling_v1
-33. ftml_update
 34. Crop
 35. ElementWiseSum
 36. diag
@@ -60,24 +56,20 @@
 38. Pad
 39. linalg_gemm2
 40. crop
-41. rmsprop_update
 43. RNN
 45. SoftmaxOutput
 46. linalg_extractdiag
-47. sgd_mom_update
 48. SequenceLast
 51. SequenceReverse
 53. SVMOutput
 54. linalg_trsm
 55. where
 56. SoftmaxActivation
-57. signsgd_update
 58. slice
 59. linalg_gelqf
 60. softmin
 61. linalg_gemm
 62. BilinearSampler
-63. mp_sgd_mom_update
 64. choose_element_0index
 65. tile
 67. gather_nd
@@ -110,7 +102,6 @@
 98. linalg_syrk
 99. squeeze
 101. ROIPooling
-102. ftrl_update
 103. SliceChannel
 104. slice_like
 106. linalg_maketrian
@@ -127,6 +118,4 @@
 119. normal
 120. take
 121. MakeLoss
-122. sgd_update
-123. adam_update
-124. concat
\ No newline at end of file
+124. concat
diff --git a/benchmark/opperf/opperf.py b/benchmark/opperf/opperf.py
index 77b16670b..b8055d7d1 100755
--- a/benchmark/opperf/opperf.py
+++ b/benchmark/opperf/opperf.py
@@ -39,6 +39,7 @@ from benchmark.opperf.nd_operations.nn_activation_operators import run_activatio
 from benchmark.opperf.nd_operations.nn_conv_operators import run_pooling_operators_benchmarks, \
     run_convolution_operators_benchmarks, run_transpose_convolution_operators_benchmarks
 from benchmark.opperf.nd_operations.nn_basic_operators import run_nn_basic_operators_benchmarks
+from benchmark.opperf.nd_operations.nn_optimizer_operators import run_optimizer_operators_benchmarks
 from benchmark.opperf.nd_operations.array_rearrange import run_rearrange_operators_benchmarks
 
 from benchmark.opperf.utils.common_utils import merge_map_list, save_to_file
@@ -96,6 +97,8 @@ def run_all_mxnet_operator_benchmarks(ctx=mx.cpu(), dtype='float32'):
     # Run all Convolution operations benchmarks with default input values
     mxnet_operator_benchmark_results.append(run_convolution_operators_benchmarks(ctx=ctx, dtype=dtype))
 
+    # Run all Optimizer operations benchmarks with default input values
+    mxnet_operator_benchmark_results.append(run_optimizer_operators_benchmarks(ctx=ctx, dtype=dtype))
     # Run all Transpose Convolution operations benchmarks with default input values
     mxnet_operator_benchmark_results.append(run_transpose_convolution_operators_benchmarks(ctx=ctx, dtype=dtype))
 
diff --git a/benchmark/opperf/rules/default_params.py b/benchmark/opperf/rules/default_params.py
index b16cd5429..c864c7d82 100644
--- a/benchmark/opperf/rules/default_params.py
+++ b/benchmark/opperf/rules/default_params.py
@@ -34,6 +34,7 @@ DEFAULT_RHS = [[(1024, 1024), (10000, 10), (10000, 1)]]
 
 # For operators like - random_uniform, random_normal etc..
 DEFAULT_SHAPE = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_SAMPLE = [(2,)]
 DEFAULT_LOW = [0]
 DEFAULT_HIGH = [5]
 DEFAULT_K = [1]
@@ -62,6 +63,31 @@ DEFAULT_AXIS_SHAPE = [(), 0, (0, 1)]
 # NOTE: Data used is DEFAULT_DATA
 DEFAULT_AXIS = [0]
 
+# For optimizer operators
+DEFAULT_WEIGHT = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_GRAD = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_MOM = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_MEAN = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_VAR = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_N = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_D = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_V = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_Z = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_G = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_DELTA = [(1024, 1024), (10000, 1), (10000, 100)]
+DEFAULT_LRS = [(0.1,0.1)]
+DEFAULT_LR = [0.1,0.5,0.9]
+DEFAULT_GAMMA_1 = [0.1,0.5,0.9]
+DEFAULT_GAMMA_2 = [0.1,0.5,0.9]
+DEFAULT_EPSILON = [1e-08]
+DEFAULT_BETA_1 = [0.1,0.5,0.9]
+DEFAULT_BETA_2 = [0.1,0.5,0.9]
+DEFAULT_T = [1,5]
+DEFAULT_RESCALE_GRAD = [0.4, 0.77]
+DEFAULT_CLIP_GRADIENT = [-1.0,0.8]
+DEFAULT_CLIP_WEIGHTS = [-1.0,0.8]
+DEFAULT_LAZY_UPDATE = [0,1]
+
 # For rearrange operators
 # NOTE: Data needs to be a 4D tensor for  operators like space_to_depth and depth_to_space
 # Hence below we append 4d to mark the difference.
@@ -71,8 +97,10 @@ DEFAULT_DIM_1 = [0, 1, 2, 3]
 DEFAULT_DIM_2 = [1, 2, 3, 0]
 DEFAULT_BLOCK_SIZE = [2, 5]
 
+
 # Default Inputs. MXNet Op Param Name to Default Input mapping
 DEFAULTS_INPUTS = {"data": DEFAULT_DATA,
+                   "sample": DEFAULT_SAMPLE,
                    "lhs": DEFAULT_LHS,
                    "rhs": DEFAULT_RHS,
                    "shape": DEFAULT_SHAPE,
@@ -91,16 +119,42 @@ DEFAULTS_INPUTS = {"data": DEFAULT_DATA,
                    "p_nd": DEFAULT_P_ND,
                    "axis_shape": DEFAULT_AXIS_SHAPE,
                    "axis": DEFAULT_AXIS,
+                   "weight" : DEFAULT_WEIGHT,
+                   "weight32" : DEFAULT_WEIGHT,
+                   "grad" : DEFAULT_GRAD,
+                   "mean" : DEFAULT_MEAN,
+                   "var" : DEFAULT_VAR,
+                   "mom" : DEFAULT_MOM,
+                   "n" : DEFAULT_N,
+                   "d" : DEFAULT_D,
+                   "v" : DEFAULT_V,
+                   "z" : DEFAULT_Z,
+                   "g" : DEFAULT_G,
+                   "delta" : DEFAULT_DELTA,
+                   "lr" : DEFAULT_LR,
+                   "lrs" : DEFAULT_LRS,
+                   "wds" : DEFAULT_LRS,
+                   "gamma1" : DEFAULT_GAMMA_1,
+                   "gamma2" : DEFAULT_GAMMA_2,
+                   "epsilon" : DEFAULT_EPSILON,
+                   "beta1" : DEFAULT_BETA_1,
+                   "beta2" : DEFAULT_BETA_2,
+                   "t" : DEFAULT_T,
+                   "rescale_grad" : DEFAULT_RESCALE_GRAD,
+                   "clip_grad" : DEFAULT_CLIP_GRADIENT,
+                   "lazy_update" : DEFAULT_LAZY_UPDATE,
                    "data_4d": DEFAULT_DATA_4d,
                    "dim1": DEFAULT_DIM_1,
                    "dim2": DEFAULT_DIM_2,
                    "block_size": DEFAULT_BLOCK_SIZE}
 
+
 # These are names of MXNet operator parameters that is of type NDArray.
 # We maintain this list to automatically recognize these parameters are to be
 # given as NDArray and translate users inputs such as a shape tuple, Numpy Array or
 # a list to MXNet NDArray. This is just a convenience added so benchmark utility users
 # can just say shape of the tensor, and we automatically create Tensors.
-PARAMS_OF_TYPE_NDARRAY = ["lhs", "rhs", "data", "base", "exp",
+PARAMS_OF_TYPE_NDARRAY = ["lhs", "rhs", "data", "base", "exp", "sample",
                           "mu", "sigma", "lam", "alpha", "beta", "gamma", "k", "p",
-                          "low", "high", "weight", "bias", "moving_mean", "moving_var"]
+                          "low", "high", "weight", "bias", "moving_mean", "moving_var",
+                          "weight", "weight32", "grad", "mean", "var", "mom", "n", "d", "v", "z", "g", "delta"]
diff --git a/benchmark/opperf/utils/op_registry_utils.py b/benchmark/opperf/utils/op_registry_utils.py
index f5e75066c..860b83a4d 100644
--- a/benchmark/opperf/utils/op_registry_utils.py
+++ b/benchmark/opperf/utils/op_registry_utils.py
@@ -244,6 +244,27 @@ def get_all_reduction_operators():
     return reduction_mx_operators
 
 
+def get_all_optimizer_operators():
+    """Gets all Optimizer operators registered with MXNet.
+
+     Returns
+     -------
+     {"operator_name": {"has_backward", "nd_op_handle", "params"}}
+     """
+    optimizer_ops = ['mp_sgd_update', 'signum_update', 'rmspropalex_update', 'ftml_update', 'rmsprop_update',
+                     'sgd_mom_update', 'signsgd_update', 'mp_sgd_mom_update', 'ftrl_update', 'sgd_update',
+                     'adam_update']
+
+    # Get all mxnet operators
+    mx_operators = _get_all_mxnet_operators()
+
+    # Filter for Optimizer operators
+    optimizer_mx_operators = {}
+    for op_name, op_params in mx_operators.items():
+         if op_name in optimizer_ops and op_name not in unique_ops:
+             optimizer_mx_operators[op_name] = mx_operators[op_name]
+    return optimizer_mx_operators
+
 def get_all_sorting_searching_operators():
     """Gets all Sorting and Searching operators registered with MXNet.
 
diff --git a/docs/_static/js/options.js b/docs/_static/js/options.js
index ca4ac363d..d400d1ce2 100644
--- a/docs/_static/js/options.js
+++ b/docs/_static/js/options.js
@@ -19,7 +19,7 @@
  */
 
 /* Installation page display functions for install selector */
-var versionSelect   = defaultVersion = 'v1.4.1';
+var versionSelect   = defaultVersion = 'v1.5.0';
 var platformSelect    = 'Linux';
 var languageSelect  = 'Python';
 var processorSelect = 'CPU';
diff --git a/docs/_static/mxnet-theme/index.html b/docs/_static/mxnet-theme/index.html
index a5f0bed63..d9311d7ae 100644
--- a/docs/_static/mxnet-theme/index.html
+++ b/docs/_static/mxnet-theme/index.html
@@ -23,9 +23,9 @@
   <div class="container">
     <div class="row">
       <div class="col-lg-4 col-sm-12">
-        <h3>MXNet 1.4.1 Released</h3>
-        <p>This patch release features bug fixes and performance improvements.</p>
-        <a href="https://github.com/apache/incubator-mxnet/releases/tag/1.4.1">Learn More</a>
+        <h3>MXNet 1.5.0 Released</h3>
+        <p>This release features Automatic Mixed Precision, MKL-DNN updates, CUDA10.1 support and more. </p>
+        <a href="https://github.com/apache/incubator-mxnet/releases/tag/1.5.0">Learn More</a>
       </div>
       <div class="col-lg-4 col-sm-12">
         <h3>A 60-minute Gluon Crash Course</h3>
diff --git a/docs/api/python/autograd/autograd.md b/docs/api/python/autograd/autograd.md
index 1905831b1..82da4eac0 100644
--- a/docs/api/python/autograd/autograd.md
+++ b/docs/api/python/autograd/autograd.md
@@ -42,16 +42,28 @@ to allocate space for the gradient. Then, start a `with autograd.record()` block
 and do some computation. Finally, call `backward()` on the result:
 
 ```python
->>> x = mx.nd.array([1,2,3,4])
->>> x.attach_grad()
->>> with mx.autograd.record():
-...     y = x * x + 1
->>> y.backward()
->>> print(x.grad)
+import mxnet as mx
+x = mx.nd.array([1,2,3,4])
+x.attach_grad()
+with mx.autograd.record():
+    y = x * x + 1
+y.backward()
+print(x.grad)
+```
+
+Which outputs:
+
+```
 [ 2.  4.  6.  8.]
 <NDArray 4 @cpu(0)>
 ```
 
+Gradient recording is enabled during the scope of the `with mx.autograd.record():` statement, then
+disabled when we go out of that scope.
+
+It can be also set manually by executing `mx.autograd.set_recording(True)`, and turning it off after
+we no longer want to record operations with `mx.autograd.set_recording(False)`.
+
 
 ## Train mode and Predict Mode
 
@@ -76,8 +88,59 @@ Detailed tutorials are available in Part 1 of
 [the MXNet gluon book](http://gluon.mxnet.io/).
 
 
+# Higher order gradient
+
+Some operators support higher order gradients. Some operators support differentiating multiple
+times, and others two, most just once.
+
+For calculating higher order gradients, we can use the `mx.autograd.grad` function while recording
+and then call backward, or call `mx.autograd.grad` two times. If we do the latter, is important that
+the first call uses `create_graph=True` and `retain_graph=True` and the second call uses
+`create_graph=False` and `retain_graph=True`. Otherwise we will not get the results that we want. If
+we would be to recreate the graph in the second call, we would end up with a graph of just the
+backward nodes, not the full initial graph that includes the forward nodes.
+
+The pattern to calculate higher order gradients is the following:
+
+```python
+from mxnet import ndarray as nd
+from mxnet import autograd as ag
+x = nd.array([1,2,3])
+x.attach_grad()
+def f(x):
+    # Any function which supports higher oder gradient
+    return nd.log(x)
+```
+
+If the operators used in `f` don't support higher order gradients you will get an error like
+`operator ... is non-differentiable because it didn't register FGradient attribute.`. This means
+that it doesn't support getting the gradient of the gradient. Which is, running backward on
+the backward graph.
+
+Using mxnet.autograd.grad multiple times:
+
+```python
+with ag.record():
+    y = f(x)
+    x_grad = ag.grad(heads=y, variables=x, create_graph=True, retain_graph=True)[0]
+    x_grad_grad = ag.grad(heads=x_grad, variables=x, create_graph=False, retain_graph=False)[0]
+```
+
+Running backward on the backward graph:
+
+```python
+with ag.record():
+    y = f(x)
+    x_grad = ag.grad(heads=y, variables=x, create_graph=True, retain_graph=True)[0]
+x_grad.backward()
+x_grad_grad = x.grad
+```
 
+Both methods are equivalent, except that in the second case, retain_graph on running backward is set
+to False by default. But both calls are running a backward pass as on the graph as usual to get the
+gradient of the first gradient `x_grad` with respect to `x` evaluated at the value of `x`.
 
+For more examples, check the [higher order gradient unit tests](https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_higher_order_grad.py).
 
 
 <script type="text/javascript" src='../../../_static/js/auto_module_index.js'></script>
diff --git a/docs/api/python/gluon/contrib.md b/docs/api/python/gluon/contrib.md
index a940f697d..22cdebb53 100644
--- a/docs/api/python/gluon/contrib.md
+++ b/docs/api/python/gluon/contrib.md
@@ -114,6 +114,33 @@ In the rest of this document, we list routines provided by the `gluon.contrib` p
     WikiText103
 ```
 
+### Estimator
+
+```eval_rst
+.. currentmodule:: mxnet.gluon.contrib.estimator
+
+.. autosummary::
+    :nosignatures:
+    
+    Estimator
+```
+
+#### EventHandler
+
+```eval_rst
+.. currentmodule:: mxnet.gluon.contrib.estimator
+
+.. autosummary::
+    :nosignatures:
+
+    StoppingHandler
+    MetricHandler
+    ValidationHandler
+    LoggingHandler
+    CheckpointHandler
+    EarlyStoppingHandler
+```
+
 ## API Reference
 
 <script type="text/javascript" src='../../../_static/js/auto_module_index.js'></script>
@@ -144,6 +171,9 @@ In the rest of this document, we list routines provided by the `gluon.contrib` p
     :members:
     :imported-members:
 
+.. automodule:: mxnet.gluon.contrib.estimator
+    :members:
+    :imported-members:
 ```
 
 <script>auto_index("api-reference");</script>
diff --git a/docs/api/scala/symbol.md b/docs/api/scala/symbol.md
index aaddc2a8a..f92548e48 100644
--- a/docs/api/scala/symbol.md
+++ b/docs/api/scala/symbol.md
@@ -41,7 +41,7 @@ The following example configures a two-layer neural network.
     val data = Symbol.Variable("data")
     val fc1 = Symbol.api.FullyConnected(Some(data), num_hidden = 128, name = "fc1")
     val act1 = Symbol.api.Activation(Some(fc1), "relu", "relu1")
-    val fc2 = Symbol.api.FullyConnected(some(act1), num_hidden = 64, name = "fc2")
+    val fc2 = Symbol.api.FullyConnected(Some(act1), num_hidden = 64, name = "fc2")
     val net = Symbol.api.SoftmaxOutput(Some(fc2), name = "out")
     :type net
     // org.apache.mxnet.Symbol
diff --git a/docs/install/download.md b/docs/install/download.md
index 808b4b8a7..ead73e5a1 100644
--- a/docs/install/download.md
+++ b/docs/install/download.md
@@ -21,6 +21,7 @@ These source archives are generated from tagged releases. Updates and patches wi
 
 | Version | Source                                                                                                      | PGP                                                                                                             | SHA                                                                                                                |
 |---------|-------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
+| 1.5.0   | [Download](https://apache.org/dist/incubator/mxnet/1.5.0/apache-mxnet-src-1.5.0-incubating.tar.gz)                 | [Download](https://apache.org/dist/incubator/mxnet/1.5.0/apache-mxnet-src-1.5.0-incubating.tar.gz.asc)    |  [Download](https://apache.org/dist/incubator/mxnet/1.5.0/apache-mxnet-src-1.5.0-incubating.tar.gz.sha512)     |
 | 1.4.1   | [Download](https://www.apache.org/dyn/closer.cgi/incubator/mxnet/1.4.1/apache-mxnet-src-1.4.1-incubating.tar.gz)   | [Download](https://apache.org/dist/incubator/mxnet/1.4.1/apache-mxnet-src-1.4.1-incubating.tar.gz.asc)    | [Download](https://apache.org/dist/incubator/mxnet/1.4.1/apache-mxnet-src-1.4.1-incubating.tar.gz.sha512)      |
 | 1.4.0   | [Download](https://www.apache.org/dyn/closer.cgi/incubator/mxnet/1.4.0/apache-mxnet-src-1.4.0-incubating.tar.gz)   | [Download](https://apache.org/dist/incubator/mxnet/1.4.0/apache-mxnet-src-1.4.0-incubating.tar.gz.asc)    | [Download](https://apache.org/dist/incubator/mxnet/1.4.0/apache-mxnet-src-1.4.0-incubating.tar.gz.sha512)      |
 | 1.3.1   | [Download](https://www.apache.org/dyn/closer.cgi/incubator/mxnet/1.3.1/apache-mxnet-src-1.3.1-incubating.tar.gz)   | [Download](https://apache.org/dist/incubator/mxnet/1.3.1/apache-mxnet-src-1.3.1-incubating.tar.gz.asc)    | [Download](https://apache.org/dist/incubator/mxnet/1.3.1/apache-mxnet-src-1.3.1-incubating.tar.gz.sha512)      |
diff --git a/docs/install/index.md b/docs/install/index.md
index ac7bd048a..db8dae8e4 100644
--- a/docs/install/index.md
+++ b/docs/install/index.md
@@ -38,10 +38,11 @@
 Indicate your preferred configuration. Then, follow the customized commands to install MXNet.
 
 <div class="dropdown">
-  <button class="btn current-version btn-primary dropdown-toggle" type="button" data-toggle="dropdown">v1.4.1
+  <button class="btn current-version btn-primary dropdown-toggle" type="button" data-toggle="dropdown">v1.5.0
   <span class="caret"></span></button>
   <ul class="dropdown-menu opt-group">
-    <li class="opt active versions"><a href="#">v1.4.1</a></li>
+    <li class="opt active versions"><a href="#">v1.5.0</a></li>
+    <li class="opt versions"><a href="#">v1.4.1</a></li>
     <li class="opt versions"><a href="#">v1.3.1</a></li>
     <li class="opt versions"><a href="#">v1.2.1</a></li>
     <li class="opt versions"><a href="#">v1.1.0</a></li>
@@ -108,7 +109,22 @@ Indicate your preferred configuration. Then, follow the customized commands to i
 </div>
 </div>
 </div>
+
+<!-- Linux Julia Options -->
+
+<div class="linux macos windows">
+<div class="julia">
+<div class="cpu gpu">
+<div class="btn-group opt-group" role="group">
+  <button type="button" class="btn btn-default environs opt active">Pkg</button>
+  <button type="button" class="btn btn-default environs opt">Build from Source</button>
+</div>
+</div>
+</div>
+</div>
+
 <hr>
+
 <!-- END - Main Menu -->
 
 <!-- START - Linux Python CPU Installation Instructions -->
@@ -117,22 +133,32 @@ Indicate your preferred configuration. Then, follow the customized commands to i
 <div class="python">
 <div class="cpu">
 <div class="pip">
-<div class="v1-4-1">
+<div class="v1-5-0">
+
+```
+$ pip install mxnet
+```
 
 MKL-DNN enabled pip packages are optimized for Intel hardware. You can find performance numbers in the <a href="http://mxnet.io/faq/perf.html#intel-cpu">MXNet tuning guide</a>.
 
 ```
-$ pip install mxnet
+$ pip install mxnet-mkl
 ```
 
-</div> <!-- End of v1-4-1 -->
-<div class="v1-4-0">
+</div> <!-- End of v1-5-0 -->
+<div class="v1-4-1">
+
+```
+$ pip install mxnet==1.4.1
+```
+
+MKL-DNN enabled pip packages are optimized for Intel hardware. You can find performance numbers in the <a href="http://mxnet.io/faq/perf.html#intel-cpu">MXNet tuning guide</a>.
 
 ```
-$ pip install mxnet==1.4.0
+$ pip install mxnet-mkl==1.4.1
 ```
 
-</div> <!-- End of v1-4-0 -->
+</div> <!-- End of v1-4-1 -->
 <div class="v1-3-1">
 
 ```
@@ -220,7 +246,7 @@ Check the chart below for other options, refer to <a href="https://pypi.org/proj
 
 **NOTES:**
 
-*mxnet-cu92mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 9.2.
+*mxnet-cu101mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 10.1.
 
 All MKL pip packages are experimental prior to version 1.3.0.
 
@@ -285,17 +311,24 @@ To build from source, refer to the <a href="ubuntu_setup.html">MXNet Ubuntu inst
 
 <div class="gpu">
 <div class="pip">
-<div class="v1-4-0">
+<div class="v1-5-0">
 
 ```
-$ pip install mxnet-cu92
+$ pip install mxnet-cu101
 ```
 
-</div> <!-- End of v1-4-0 -->
+</div> <!-- End of v1-5-0 -->
+<div class="v1-4-1">
+
+```
+$ pip install mxnet-cu101==1.4.1
+```
+
+</div> <!-- End of v1-4-1 -->
 <div class="v1-3-1">
 
 ```
-$ pip install mxnet==1.3.1
+$ pip install mxnet-cu92==1.3.1
 ```
 
 </div> <!-- End of v1-3-1-->
@@ -342,7 +375,7 @@ $ pip install mxnet-cu80==0.11.0
 <div class="master">
 
 ```
-$ pip install mxnet-cu92 --pre
+$ pip install mxnet-cu101 --pre
 ```
 
 </div> <!-- End of master-->
@@ -354,7 +387,7 @@ Check the chart below for other options, refer to <a href="https://pypi.org/proj
 
 **NOTES:**
 
-*mxnet-cu92mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 9.2.
+*mxnet-cu101mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 10.1.
 
 All MKL pip packages are experimental prior to version 1.3.0.
 
@@ -531,13 +564,13 @@ You can use the Maven packages defined in the following dependency to include MX
 <br/>
 You can use the Maven packages defined in the following dependency to include MXNet in your Java project. The Java API is provided as a subset of the Scala API and is intended for inference only. Please refer to the <a href="java_setup.html">MXNet-Java setup guide</a> for a detailed set of instructions to help you with the setup process.
 
-<a href="https://repository.apache.org/#nexus-search;gav~org.apache.mxnet~~1.4.1~~"><img src="https://img.shields.io/badge/org.apache.mxnet-linux gpu-green.svg" alt="maven badge"/></a>
+<a href="https://repository.apache.org/#nexus-search;gav~org.apache.mxnet~~1.5.0~~"><img src="https://img.shields.io/badge/org.apache.mxnet-linux gpu-green.svg" alt="maven badge"/></a>
 
 ```html
 <dependency>
     <groupId>org.apache.mxnet</groupId>
     <artifactId>mxnet-full_2.11-linux-x86_64-gpu</artifactId>
-    <version>[1.4.1, )</version>
+    <version>[1.5.0, )</version>
 </dependency>
 ```
 
@@ -548,13 +581,13 @@ You can use the Maven packages defined in the following dependency to include MX
 <br/>
 You can use the Maven packages defined in the following dependency to include MXNet in your Java project. The Java API is provided as a subset of the Scala API and is intended for inference only. Please refer to the <a href="java_setup.html">MXNet-Java setup guide</a> for a detailed set of instructions to help you with the setup process.
 
-<a href="https://repository.apache.org/#nexus-search;gav~org.apache.mxnet~~1.4.1~~"><img src="https://img.shields.io/badge/org.apache.mxnet-linux cpu-green.svg" alt="maven badge"/></a>
+<a href="https://repository.apache.org/#nexus-search;gav~org.apache.mxnet~~1.5.0~~"><img src="https://img.shields.io/badge/org.apache.mxnet-linux cpu-green.svg" alt="maven badge"/></a>
 
 ```html
 <dependency>
     <groupId>org.apache.mxnet</groupId>
     <artifactId>mxnet-full_2.11-linux-x86_64-cpu</artifactId>
-    <version>[1.4.1, )</version>
+    <version>[1.5.0, )</version>
 </dependency>
 ```
 <br>
@@ -565,8 +598,23 @@ You can use the Maven packages defined in the following dependency to include MX
 <div class="julia">
 <div class="cpu gpu">
 </br>
+<div class="pkg">
+
+```
+]add MXNet#v1.5.0
+```
+
+Install the latest release:
+
+```
+]add MXNet
+```
+
+</div>
+<div class="build-from-source">
 Refer to the <a href="ubuntu_setup.html#install-the-mxnet-package-for-julia">Julia section of the MXNet Ubuntu installation guide</a>.
 
+</div>
 </div> <!-- End of cpu gpu -->
 </div> <!-- End of julia -->
 
@@ -576,7 +624,7 @@ Refer to the <a href="ubuntu_setup.html#install-the-mxnet-package-for-julia">Jul
 Refer to the <a href="ubuntu_setup.html#install-the-mxnet-package-for-perl">Perl section of the MXNet Ubuntu installation guide</a>.
 
 </div> <!-- End of cpu gpu -->
-</div> <!-- End of julia -->
+</div> <!-- End of perl -->
 
 
 
@@ -599,12 +647,18 @@ Refer to the <a href="ubuntu_setup.html#install-the-mxnet-package-for-perl">Perl
 <div class="python">
 <div class="cpu">
 <div class="pip">
-<div class="v1-4-0">
+<div class="v1-5-0">
 
 ```
 $ pip install mxnet
 ```
-</div> <!-- End of v1-4-0 -->
+</div> <!-- End of v1-5-0 -->
+<div class="v1-4-1">
+
+```
+$ pip install mxnet==1.4.1
+```
+</div> <!-- End of v1-4-1 -->
 <div class="v1-3-1">
 
 ```
@@ -670,7 +724,7 @@ Check the chart below for other options, refer to <a href="https://pypi.org/proj
 
 **NOTES:**
 
-*mxnet-cu92mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 9.2.
+*mxnet-cu101mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 10.1.
 
 All MKL pip packages are experimental prior to version 1.3.0.
 
@@ -839,13 +893,13 @@ Not available at this time. <br>
 </br>
 You can use the Maven packages defined in the following dependency to include MXNet in your Java project. The Java API is provided as a subset of the Scala API and is intended for inference only. Please refer to the <a href="java_setup.html">MXNet-Java setup guide</a> for a detailed set of instructions to help you with the setup process.
 
-<a href="https://repository.apache.org/#nexus-search;gav~org.apache.mxnet~~1.4.1~~"><img src="https://img.shields.io/badge/org.apache.mxnet-mac cpu-green.svg" alt="maven badge"/></a>
+<a href="https://repository.apache.org/#nexus-search;gav~org.apache.mxnet~~1.5.0~~"><img src="https://img.shields.io/badge/org.apache.mxnet-mac cpu-green.svg" alt="maven badge"/></a>
 
 ```html
 <dependency>
     <groupId>org.apache.mxnet</groupId>
     <artifactId>mxnet-full_2.11-linux-x86_64-cpu</artifactId>
-    <version>[1.4.1, )</version>
+    <version>[1.5.0, )</version>
 </dependency>
 ```
 <br>
@@ -861,8 +915,23 @@ Not available at this time. <br>
 <div class="julia">
 <div class="cpu gpu">
 </br>
+<div class="pkg">
+
+```
+]add MXNet#v1.5.0
+```
+
+Install the latest release:
+
+```
+]add MXNet
+```
+
+</div>
+<div class="build-from-source">
 Refer to the <a href="osx_setup.html#install-the-mxnet-package-for-julia">Julia section of the MXNet macOS installation guide</a>.
 
+</div>
 </div> <!-- End of cpu gpu -->
 </div> <!-- End of julia -->
 
@@ -892,13 +961,20 @@ For more installation options, refer to the <a href="osx_setup.html">MXNet macOS
 <div class="python">
 <div class="cpu">
 <div class="pip">
-<div class="v1-4-0">
+<div class="v1-5-0">
 
 ```
 $ pip install mxnet
 ```
 
-</div> <!-- End of v1-4-0 -->
+</div> <!-- End of v1-5-0 -->
+<div class="v1-4-1">
+
+```
+$ pip install mxnet==1.4.1
+```
+
+</div> <!-- End of v1-4-1 -->
 <div class="v1-3-1">
 
 ```
@@ -961,7 +1037,7 @@ Check the chart below for other options, refer to <a href="https://pypi.org/proj
 
 **NOTES:**
 
-*mxnet-cu92mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 9.2.
+*mxnet-cu101mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 10.1.
 
 All MKL pip packages are experimental prior to version 1.3.0.
 
@@ -1023,17 +1099,24 @@ Refer to the <a href="windows_setup.html">MXNet Windows installation guide</a>
 
 <div class="gpu">
 <div class="pip">
-<div class="v1-4-0">
+<div class="v1-5-0">
 
 ```
-$ pip install mxnet-cu92
+$ pip install mxnet-cu101
 ```
 
-</div> <!-- End of v1-4-0 -->
+</div> <!-- End of v1-5-0 -->
+<div class="v1-4-1">
+
+```
+$ pip install mxnet-cu101==1.4.1
+```
+
+</div> <!-- End of v1-4-1 -->
 <div class="v1-3-1">
 
 ```
-$ pip install mxnet==1.3.1
+$ pip install mxnet-cu92==1.3.1
 ```
 
 </div> <!-- End of v1-3-1 -->
@@ -1080,7 +1163,7 @@ $ pip install mxnet-cu80==0.11.0
 <div class="master">
 
 ```
-$ pip install mxnet-cu92 --pre
+$ pip install mxnet-cu101 --pre
 ```
 
 </div> <!-- End of master-->
@@ -1092,7 +1175,7 @@ Check the chart below for other options, refer to <a href="https://pypi.org/proj
 
 **NOTES:**
 
-*mxnet-cu92mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 9.2.
+*mxnet-cu101mkl* means the package is built with CUDA/cuDNN and MKL-DNN enabled and the CUDA version is 10.1.
 
 All MKL pip packages are experimental prior to version 1.3.0.
 
@@ -1189,8 +1272,23 @@ MXNet-Java for Windows is not yet available.
 <div class="julia">
 <div class="cpu gpu">
 </br>
+<div class="pkg">
+
+```
+]add MXNet#v1.5.0
+```
+
+Install the latest release:
+
+```
+]add MXNet
+```
+
+</div>
+<div class="build-from-source">
 Refer to the <a href="windows_setup.html#install-the-mxnet-package-for-julia">Julia section of the MXNet Windows installation guide</a>.
 
+</div>
 </div> <!-- End of cpu gpu -->
 </div> <!-- End of julia -->
 
@@ -1200,7 +1298,7 @@ Refer to the <a href="windows_setup.html#install-the-mxnet-package-for-julia">Ju
 Refer to the <a href="windows_setup.html#install-the-mxnet-package-for-perl">Perl section of the MXNet Windows installation guide</a>.
 
 </div> <!-- End of cpu gpu -->
-</div> <!-- End of julia -->
+</div> <!-- End of perl -->
 
 <div class="cpp">
 <div class="cpu gpu">
diff --git a/docs/tutorials/index.md b/docs/tutorials/index.md
index 6e31e825e..f773a79f6 100644
--- a/docs/tutorials/index.md
+++ b/docs/tutorials/index.md
@@ -105,6 +105,8 @@ Select API:&nbsp;
     * [Module to Gluon API](/tutorials/python/module_to_gluon.html)
     * [Gluon end to end from training to inference](/tutorials/gluon/gluon_from_experiment_to_deployment.html)
     * [Automatic Mixed Precision in Gluon](/tutorials/amp/amp_tutorial.html)
+    * [How to build and install MXNet with MKL-DNN backend](/tutorials/mkldnn/MKLDNN_README.html)
+    * [How to quantize custom models with MKL-DNN backend](/tutorials/mkldnn/mkldnn_quantization.html)<span style="color:red"> (new!) </span>
 * API Guides
     * Core APIs
         * NDArray
@@ -137,6 +139,8 @@ Select API:&nbsp;
             * [Data Transforms](/tutorials/gluon/transforms.html)
             * [Applying Data Augmentation](/tutorials/gluon/data_augmentation.html)
             * [Data Augmentation with Masks (for Object Segmentation)](https://mxnet.incubator.apache.org/tutorials/python/data_augmentation_with_masks.html)
+        * Fit API
+            * [Using Fit API](/tutorials/gluon/fit_api_tutorial.html)
 </div> <!--end of gluon-->
 
 <div class="module">
@@ -157,7 +161,6 @@ Select API:&nbsp;
     * [Large-Scale Multi-Host Multi-GPU Image Classification](/tutorials/vision/large_scale_classification.html)
     * [Importing an ONNX model into MXNet](/tutorials/onnx/super_resolution.html)
     * [Optimizing Deep Learning Computation Graphs with TensorRT](/tutorials/tensorrt/inference_with_trt.html)
-    * [How to build and install MXNet with MKL-DNN backend](/tutorials/mkldnn/MKLDNN_README.html)
 * API Guides
     * Core APIs
         * NDArray
diff --git a/docs/tutorials/python/profiler.md b/docs/tutorials/python/profiler.md
index 5be6bf83a..91a74e4f4 100644
--- a/docs/tutorials/python/profiler.md
+++ b/docs/tutorials/python/profiler.md
@@ -195,10 +195,10 @@ print(profiler.dumps())
 You can also dump the information collected by the profiler into a `json` file using the `profiler.dump()` function and view it in a browser.
 
 ```python
-profiler.dump()
+profiler.dump(finished=False)
 ```
 
-`dump()` creates a `json` file which can be viewed using a trace consumer like `chrome://tracing` in the Chrome browser. Here is a snapshot that shows the output of the profiling we did above.
+`dump()` creates a `json` file which can be viewed using a trace consumer like `chrome://tracing` in the Chrome browser. Here is a snapshot that shows the output of the profiling we did above. Note that setting the `finished` parameter to `False` will prevent the profiler from finishing dumping to file. If you just use `profiler.dump()`, you will no longer be able to profile the remaining sections of your model. 
 
 ![Tracing Screenshot](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tutorials/python/profiler/profiler_output_chrome.png)
 
@@ -214,11 +214,6 @@ Should the existing NDArray operators fail to meet all your model's needs, MXNet
 Let's try profiling custom operators with the following code example:
 
 ```python
-
-import mxnet as mx
-from mxnet import nd
-from mxnet import profiler
-
 class MyAddOne(mx.operator.CustomOp):
     def forward(self, is_train, req, in_data, out_data, aux):  
         self.assign(out_data[0], req[0], in_data[0]+1)
@@ -246,7 +241,8 @@ class CustomAddOneProp(mx.operator.CustomOpProp):
 
 inp = mx.nd.zeros(shape=(500, 500))
 
-profiler.set_config(profile_all=True, continuous_dump = True)
+profiler.set_config(profile_all=True, continuous_dump=True, \
+                    aggregate_stats=True)
 profiler.set_state('run')
 
 w = nd.Custom(inp, op_type="MyAddOne")
@@ -254,7 +250,8 @@ w = nd.Custom(inp, op_type="MyAddOne")
 mx.nd.waitall()
 
 profiler.set_state('stop')
-profiler.dump()
+print(profiler.dumps())
+profiler.dump(finished=False)
 ```
 
 Here, we have created a custom operator called `MyAddOne`, and within its `forward()` function, we simply add one to the input. We can visualize the dump file in `chrome://tracing/`:
@@ -267,10 +264,10 @@ Please note that: to be able to see the previously described information, you ne
 
 ```python 
 # Set profile_all to True
-profiler.set_config(profile_all=True, aggregate_stats=True, continuous_dump = True)
+profiler.set_config(profile_all=True, aggregate_stats=True, continuous_dump=True)
 # OR, Explicitly Set profile_symbolic and profile_imperative to True
-profiler.set_config(profile_symbolic = True, profile_imperative = True, \
-    aggregate_stats=True, continuous_dump = True)
+profiler.set_config(profile_symbolic=True, profile_imperative=True, \
+                    aggregate_stats=True, continuous_dump=True)
 
 profiler.set_state('run')
 # Use Symbolic Mode
@@ -280,9 +277,15 @@ c = b.bind(mx.cpu(), {'a': inp})
 y = c.forward()
 mx.nd.waitall()
 profiler.set_state('stop')
+print(profiler.dumps())
 profiler.dump()
 ```
 
+### Some Rules to Pay Attention to
+1. Always use `profiler.dump(finished=False)` if you do not intend to finish dumping to file. Otherwise, calling `profiler.dump()` in the middle of your model may lead to unexpected behaviors; and if you subsequently call `profiler.set_config()`, the program will error out.
+
+2. You can only dump to one file. Do not change the target file by calling `profiler.set_config(filename='new_name.json')` in the middle of your model. This will lead to incomplete dump outputs.
+
 ## Advanced: Using NVIDIA Profiling Tools
 
 MXNet's Profiler is the recommended starting point for profiling MXNet code, but NVIDIA also provides a couple of tools for low-level profiling of CUDA code: [NVProf](https://devblogs.nvidia.com/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/), [Visual Profiler](https://developer.nvidia.com/nvidia-visual-profiler) and [Nsight Compute](https://developer.nvidia.com/nsight-compute). You can use these tools to profile all kinds of executables, so they can be used for profiling Python scripts running MXNet. And you can use these in conjunction with the MXNet Profiler to see high-level information from MXNet alongside the low-level CUDA kernel information.
diff --git a/example/README.md b/example/README.md
index be9903099..2b9c36684 100644
--- a/example/README.md
+++ b/example/README.md
@@ -151,7 +151,7 @@ If your tutorial depends on specific packages, simply add them to this provision
 
 ### <a name="deep-learning-examples-other"></a>Other Deep Learning Examples with MXNet
 
-* [Face Recognition with ArcFace](https://github.com/onnx/models/tree/master/models/face_recognition/ArcFace) - ONNX model for face recognition with notebooks for training, validating and running inference in MXNet by [abhinavs95](https://github.com/abhinavs95)
+* [Face Recognition with ArcFace](https://github.com/onnx/models/tree/master/vision/body_analysis/arcface) - ONNX model for face recognition with notebooks for training, validating and running inference in MXNet by [abhinavs95](https://github.com/abhinavs95)
 * [Chinese plate recognition](https://github.com/imistyrain/mxnet-mr) - Recognize Chinese vehicle plate, by [imistyrain](https://github.com/imistyrain)
 * [Fast R-CNN](https://github.com/precedenceguo/mx-rcnn) by [Jian Guo](https://github.com/precedenceguo)
 * "End2End Captcha Recognition (OCR)" by [xlvector](https://github.com/xlvector) [github link](https://github.com/xlvector/learning-dl/tree/master/mxnet/ocr) [Blog in Chinese](http://blog.xlvector.net/2016-05/mxnet-ocr-cnn/)
diff --git a/example/quantization/README.md b/example/quantization/README.md
index 09321beb7..1ae58fbb3 100644
--- a/example/quantization/README.md
+++ b/example/quantization/README.md
@@ -9,13 +9,76 @@ This folder contains examples of quantizing a FP32 model with Intel® MKL-DNN or
 
 <h2 id="1">Model Quantization with Intel® MKL-DNN</h2>
 
-Intel® MKL-DNN supports quantization with subgraph features on Intel® CPU Platform and can bring performance improvements on the [Intel® Xeon® Scalable Platform](https://www.intel.com/content/www/us/en/processors/xeon/scalable/xeon-scalable-platform.html). A new quantization script `imagenet_gen_qsym_mkldnn.py` has been designed to launch quantization for CNN models with Intel® MKL-DNN. This script integrates with [Gluon-CV modelzoo](https://gluon-cv.mxnet.io/model_zoo/classification.html), so that more pre-trained models can be downloaded from Gluon-CV and then converted for quantization. This script also supports custom models.
-
-Calibration is used for generating a calibration table for the quantized symbol. The quantization script supports three methods:
-
-- **none:** No calibration will be used. The thresholds for quantization will be calculated on the fly. This will result in inference speed slowdown and loss of accuracy in general.
-- **naive:** Simply take min and max values of layer outputs as thresholds for quantization. In general, the inference accuracy worsens with more examples used in calibration. It is recommended to use `entropy` mode as it produces more accurate inference results.
-- **entropy:** Calculate KL divergence of the fp32 output and quantized output for optimal thresholds. This mode is expected to produce the best inference accuracy of all three kinds of quantized models if the calibration dataset is representative enough of the inference dataset.
+Intel® MKL-DNN supports quantization with subgraph features on Intel® CPU Platform and can bring performance improvements on the [Intel® Xeon® Scalable Platform](https://www.intel.com/content/www/us/en/processors/xeon/scalable/xeon-scalable-platform.html). A new quantization script `imagenet_gen_qsym_mkldnn.py` has been designed to launch quantization for image-classification models with Intel® MKL-DNN. This script integrates with [Gluon-CV modelzoo](https://gluon-cv.mxnet.io/model_zoo/classification.html), so that more pre-trained models can be downloaded from Gluon-CV and then converted for quantization. To apply quantization flow to your project directly, please refer [Quantize custom models with MKL-DNN backend](https://mxnet.incubator.apache.org/tutorials/mkldnn/mkldnn_quantization.html).
+
+```
+usage: imagenet_gen_qsym_mkldnn.py [-h] [--model MODEL] [--epoch EPOCH]
+                                   [--no-pretrained] [--batch-size BATCH_SIZE]
+                                   [--label-name LABEL_NAME]
+                                   [--calib-dataset CALIB_DATASET]
+                                   [--image-shape IMAGE_SHAPE]
+                                   [--data-nthreads DATA_NTHREADS]
+                                   [--num-calib-batches NUM_CALIB_BATCHES]
+                                   [--exclude-first-conv] [--shuffle-dataset]
+                                   [--shuffle-chunk-seed SHUFFLE_CHUNK_SEED]
+                                   [--shuffle-seed SHUFFLE_SEED]
+                                   [--calib-mode CALIB_MODE]
+                                   [--quantized-dtype {auto,int8,uint8}]
+                                   [--enable-calib-quantize ENABLE_CALIB_QUANTIZE]
+
+Generate a calibrated quantized model from a FP32 model with Intel MKL-DNN
+support
+
+optional arguments:
+  -h, --help            show this help message and exit
+  --model MODEL         model to be quantized.
+  --epoch EPOCH         number of epochs, default is 0
+  --no-pretrained       If enabled, will not download pretrained model from
+                        MXNet or Gluon-CV modelzoo.
+  --batch-size BATCH_SIZE
+  --label-name LABEL_NAME
+  --calib-dataset CALIB_DATASET
+                        path of the calibration dataset
+  --image-shape IMAGE_SHAPE
+  --data-nthreads DATA_NTHREADS
+                        number of threads for data decoding
+  --num-calib-batches NUM_CALIB_BATCHES
+                        number of batches for calibration
+  --exclude-first-conv  excluding quantizing the first conv layer since the
+                        input data may have negative value which doesn't
+                        support at moment
+  --shuffle-dataset     shuffle the calibration dataset
+  --shuffle-chunk-seed SHUFFLE_CHUNK_SEED
+                        shuffling chunk seed, see https://mxnet.incubator.apac
+                        he.org/api/python/io/io.html?highlight=imager#mxnet.io
+                        .ImageRecordIter for more details
+  --shuffle-seed SHUFFLE_SEED
+                        shuffling seed, see https://mxnet.incubator.apache.org
+                        /api/python/io/io.html?highlight=imager#mxnet.io.Image
+                        RecordIter for more details
+  --calib-mode CALIB_MODE
+                        calibration mode used for generating calibration table
+                        for the quantized symbol; supports 1. none: no
+                        calibration will be used. The thresholds for
+                        quantization will be calculated on the fly. This will
+                        result in inference speed slowdown and loss of
+                        accuracy in general. 2. naive: simply take min and max
+                        values of layer outputs as thresholds for
+                        quantization. In general, the inference accuracy
+                        worsens with more examples used in calibration. It is
+                        recommended to use `entropy` mode as it produces more
+                        accurate inference results. 3. entropy: calculate KL
+                        divergence of the fp32 output and quantized output for
+                        optimal thresholds. This mode is expected to produce
+                        the best inference accuracy of all three kinds of
+                        quantized models if the calibration dataset is
+                        representative enough of the inference dataset.
+  --quantized-dtype {auto,int8,uint8}
+                        quantization destination data type for input data
+  --enable-calib-quantize ENABLE_CALIB_QUANTIZE
+                        If enabled, the quantize op will be calibrated offline
+                        if calibration mode is enabled
+```
 
 Use the following command to install [Gluon-CV](https://gluon-cv.mxnet.io/):
 
@@ -23,12 +86,13 @@ Use the following command to install [Gluon-CV](https://gluon-cv.mxnet.io/):
 pip install gluoncv
 ```
 
-The following models have been tested on Linux systems.
+Below are some quantization demos. These models have been tested on Linux systems.
 
 | Model | Source | Dataset | FP32 Accuracy (top-1/top-5)| INT8 Accuracy (top-1/top-5)|
 |:---|:---|---|:---:|:---:|
 | [ResNet18-V1](#3)  | [Gluon-CV](https://gluon-cv.mxnet.io/model_zoo/classification.html)  | [Validation Dataset](http://data.mxnet.io/data/val_256_q90.rec)  |70.15%/89.38%|69.92%/89.26%|
 | [ResNet50-V1](#3)  | [Gluon-CV](https://gluon-cv.mxnet.io/model_zoo/classification.html)  | [Validation Dataset](http://data.mxnet.io/data/val_256_q90.rec)  | 76.34%/93.13%  |  75.91%/92.95% |
+| [ResNet50-V1b](#3)  | [Gluon-CV](https://gluon-cv.mxnet.io/model_zoo/classification.html)  | [Validation Dataset](http://data.mxnet.io/data/val_256_q90.rec)  | 76.82%/93.38% |  76.39%/93.24% |
 | [ResNet101-V1](#3)  | [Gluon-CV](https://gluon-cv.mxnet.io/model_zoo/classification.html)  | [Validation Dataset](http://data.mxnet.io/data/val_256_q90.rec)  | 77.33%/93.59%  | 77.05%/93.43%  |
 |[Squeezenet 1.0](#4)|[Gluon-CV](https://gluon-cv.mxnet.io/model_zoo/classification.html)|[Validation Dataset](http://data.mxnet.io/data/val_256_q90.rec)|56.98%/79.20%|52.98%/77.21%|
 |[MobileNet 1.0](#5)|[Gluon-CV](https://gluon-cv.mxnet.io/model_zoo/classification.html)|[Validation Dataset](http://data.mxnet.io/data/val_256_q90.rec)|72.23%/90.64%|72.03%/90.42%|
@@ -39,7 +103,7 @@ The following models have been tested on Linux systems.
 | [SSD-VGG16](#10) | [example/ssd](https://github.com/apache/incubator-mxnet/tree/master/example/ssd)  | VOC2007/2012  | 0.8366 mAP  | 0.8364 mAP  |
 | [SSD-VGG16](#10) | [example/ssd](https://github.com/apache/incubator-mxnet/tree/master/example/ssd)  | COCO2014  | 0.2552 mAP  | 0.253 mAP  |
 
-<h3 id='3'>ResNet18/50/101-V1</h3>
+<h3 id='3'>ResNetV1</h3>
 
 The following command is to download the pre-trained model from Gluon-CV and transfer it into the symbolic model which would be finally quantized. The [validation dataset](http://data.mxnet.io/data/val_256_q90.rec) is available for testing the pre-trained models:
 
@@ -47,7 +111,7 @@ The following command is to download the pre-trained model from Gluon-CV and tra
 python imagenet_gen_qsym_mkldnn.py --model=resnet50_v1 --num-calib-batches=5 --calib-mode=naive
 ```
 
-The model would be automatically replaced in fusion and quantization format. It is then saved as the quantized symbol and parameter files in the `./model` directory. The following command is to launch inference.
+The model would be automatically replaced in fusion and quantization format. It is then saved as the quantized symbol and parameter files in the `./model` directory. Set `--model` to `resnet18_v1/resnet50_v1b/resnet101_v1` to quantize other models. The following command is to launch inference.
 
 ```
 # Launch FP32 Inference
@@ -204,17 +268,14 @@ SSD model is located in [example/ssd](https://github.com/apache/incubator-mxnet/
 This script also supports custom symbolic models. You can easily add some quantization layer configs in `imagenet_gen_qsym_mkldnn.py` like below:
 
 ```
-elif args.model == 'custom':
+else:
+    logger.info('Please set proper RGB configs for model %s' % args.model)
     # add rgb mean/std of your model.
     rgb_mean = '0,0,0'
     rgb_std = '0,0,0'
-    calib_layer = lambda name: name.endswith('_output')
     # add layer names you donnot want to quantize.
-    # add conv/pool layer names that has negative inputs
-    # since Intel® MKL-DNN only support uint8 quantization temporary.
-    # add all fc layer names since Intel® MKL-DNN does not support temporary.
+    logger.info('Please set proper excluded_sym_names for model %s' % args.model)
     excluded_sym_names += ['layers']
-    # add your first conv layer names since Intel® MKL-DNN only support uint8 quantization temporary.
     if exclude_first_conv:
         excluded_sym_names += ['layers']
 ```
@@ -230,7 +291,7 @@ Some tips on quantization configs:
 python imagenet_inference.py --symbol-file=./model/custom-symbol.json --param-file=./model/custom-0000.params --rgb-mean=* --rgb-std=* --num-skipped-batches=* --batch-size=* --num-inference-batches=*--dataset=./data/* --ctx=cpu
 ```
 
-3. Then, you should add `rgb_mean`, `rgb_std` and `excluded_sym_names` in this script. Notice that you should exclude conv/pool layers that have negative data since Intel® MKL-DNN only supports `uint8` quantization temporarily. You should also exclude all fc layers in your model.
+3. Then, you should add `rgb_mean`, `rgb_std` and `excluded_sym_names` in this script.
 
 4. Then, you can run the following command for quantization:
 
diff --git a/example/quantization/imagenet_gen_qsym_mkldnn.py b/example/quantization/imagenet_gen_qsym_mkldnn.py
index 482127ba3..302a04449 100644
--- a/example/quantization/imagenet_gen_qsym_mkldnn.py
+++ b/example/quantization/imagenet_gen_qsym_mkldnn.py
@@ -92,21 +92,12 @@ def save_params(fname, arg_params, aux_params, logger=None):
 
 if __name__ == '__main__':
     parser = argparse.ArgumentParser(description='Generate a calibrated quantized model from a FP32 model with Intel MKL-DNN support')
-    parser.add_argument('--model', type=str, choices=['resnet18_v1',
-                                                      'resnet50_v1',
-                                                      'resnet101_v1',
-                                                      'inceptionv3',
-                                                      'squeezenet1.0',
-                                                      'mobilenet1.0',
-                                                      'mobilenetv2_1.0',
-                                                      'imagenet1k-resnet-152',
-                                                      'imagenet1k-inception-bn',
-                                                      'custom'],
-                        help='currently only supports imagenet1k-resnet-50_v1, imagenet1k-resnet-152 or imagenet1k-inception-bn.'
-                             'you can set to custom to load your pre-trained model.')
-    parser.add_argument('--use-gluon-model', type=bool, default=False,
-                        help='If enabled, will download pretrained model from Gluon-CV '
-                             'and convert to symbolic model ')
+    parser.add_argument('--model', type=str, default='resnet50_v1',
+                        help='model to be quantized.')
+    parser.add_argument('--epoch', type=int, default=0,
+                        help='number of epochs, default is 0')
+    parser.add_argument('--no-pretrained', action='store_true', default=False,
+                        help='If enabled, will not download pretrained model from MXNet or Gluon-CV modelzoo.')
     parser.add_argument('--batch-size', type=int, default=32)
     parser.add_argument('--label-name', type=str, default='softmax_label')
     parser.add_argument('--calib-dataset', type=str, default='data/val_256_q90.rec',
@@ -155,6 +146,7 @@ if __name__ == '__main__':
     logger = logging.getLogger('logger')
     logger.setLevel(logging.INFO)
 
+    logger.info(args)
     logger.info('shuffle_dataset=%s' % args.shuffle_dataset)
 
     calib_mode = args.calib_mode
@@ -165,29 +157,24 @@ if __name__ == '__main__':
         download_calib_dataset('http://data.mxnet.io/data/val_256_q90.rec', args.calib_dataset)
 
     # download model
-    if args.model in ['resnet18_v1',
-                      'resnet50_v1',
-                      'resnet101_v1',
-                      'squeezenet1.0',
-                      'mobilenet1.0',
-                      'mobilenetv2_1.0',
-                      'inceptionv3']:
-        logger.info('model %s is converted from GluonCV' % args.model)
-        args.use_gluon_model = True
-    if args.use_gluon_model == True:
-        prefix = convert_from_gluon(model_name=args.model, image_shape=args.image_shape, classes=1000, logger=logger)
-        epoch = 0
-        sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
-    elif args.model == 'custom':
+    if not args.no_pretrained:
+        logger.info('Get pre-trained model from MXNet or Gluoncv modelzoo.')
+        logger.info('If you want to use custom model, please set --no-pretrained.')
+        if args.model in ['imagenet1k-resnet-152', 'imagenet1k-inception-bn']:
+            logger.info('model %s is downloaded from MXNet modelzoo' % args.model)
+            prefix, epoch = download_model(model_name=args.model, logger=logger)
+        else:
+            logger.info('model %s is converted from GluonCV' % args.model)
+            prefix = convert_from_gluon(model_name=args.model, image_shape=args.image_shape, classes=1000, logger=logger)
+            rgb_mean = '123.68,116.779,103.939'
+            rgb_std = '58.393, 57.12, 57.375'
+            epoch = 0
+    else:
         dir_path = os.path.dirname(os.path.realpath(__file__))
         prefix = os.path.join(dir_path, 'model', args.model)
-        epoch = 0
-        sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
-    else:
-        prefix, epoch = download_model(model_name=args.model, logger=logger)
-        sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
+        epoch = args.epoch
 
-    sym = sym.get_backend_symbol('MKLDNN_QUANTIZE')
+    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
 
     # get batch size
     batch_size = args.batch_size
@@ -212,57 +199,59 @@ if __name__ == '__main__':
         logger.info('quantized dtype is set to uint8, will exclude first conv.')
         exclude_first_conv = True
     excluded_sym_names = []
-    if args.model == 'imagenet1k-resnet-152':
-        rgb_mean = '0,0,0'
-        rgb_std = '1,1,1'
-        excluded_sym_names += ['flatten0']
-        if exclude_first_conv:
-            excluded_sym_names += ['conv0']
-    elif args.model == 'imagenet1k-inception-bn':
-        rgb_mean = '123.68,116.779,103.939'
-        rgb_std = '1,1,1'
-        excluded_sym_names += ['flatten']
-        if exclude_first_conv:
-            excluded_sym_names += ['conv_1']
-    elif args.model in ['resnet18_v1', 'resnet50_v1', 'resnet101_v1']:
-        rgb_mean = '123.68,116.779,103.939'
-        rgb_std = '58.393, 57.12, 57.375'
-        if exclude_first_conv:
-            excluded_sym_names += ['resnetv10_conv0_fwd']
-    elif args.model == 'squeezenet1.0':
-        rgb_mean = '123.68,116.779,103.939'
-        rgb_std = '58.393, 57.12, 57.375'
-        excluded_sym_names += ['squeezenet0_flatten0_flatten0']
-        if exclude_first_conv:
-            excluded_sym_names += ['squeezenet0_conv0_fwd']
-    elif args.model == 'mobilenet1.0':
-        rgb_mean = '123.68,116.779,103.939'
-        rgb_std = '58.393, 57.12, 57.375'
-        excluded_sym_names += ['mobilenet0_flatten0_flatten0',
-                               'mobilenet0_pool0_fwd']
-        if exclude_first_conv:
-            excluded_sym_names += ['mobilenet0_conv0_fwd']
-    elif args.model == 'mobilenetv2_1.0':
-        rgb_mean = '123.68,116.779,103.939'
-        rgb_std = '58.393, 57.12, 57.375'
-        excluded_sym_names += ['mobilenetv20_output_flatten0_flatten0']
-        if exclude_first_conv:
-            excluded_sym_names += ['mobilenetv20_conv0_fwd']
-    elif args.model == 'inceptionv3':
-        rgb_mean = '123.68,116.779,103.939'
-        rgb_std = '58.393, 57.12, 57.375'
-        if exclude_first_conv:
-            excluded_sym_names += ['inception30_conv0_fwd']
-    elif args.model == 'custom':
+    if not args.no_pretrained:
+        if args.model == 'imagenet1k-resnet-152':
+            rgb_mean = '0,0,0'
+            rgb_std = '1,1,1'
+            excluded_sym_names += ['flatten0']
+            if exclude_first_conv:
+                excluded_sym_names += ['conv0']
+        elif args.model == 'imagenet1k-inception-bn':
+            rgb_mean = '123.68,116.779,103.939'
+            rgb_std = '1,1,1'
+            excluded_sym_names += ['flatten']
+            if exclude_first_conv:
+                excluded_sym_names += ['conv_1']
+        elif args.model.find('resnet') != -1 and args.model.find('v1') != -1:
+            if exclude_first_conv:
+                excluded_sym_names += ['resnetv10_conv0_fwd']
+        elif args.model.find('resnet') != -1 and args.model.find('v2') != -1:
+            excluded_sym_names += ['resnetv20_flatten0_flatten0']
+            if exclude_first_conv:
+                excluded_sym_names += ['resnetv20_conv0_fwd']
+        elif args.model.find('vgg') != -1:
+            if exclude_first_conv:
+                excluded_sym_names += ['vgg0_conv0_fwd']
+        elif args.model.find('squeezenet1') != -1:
+            excluded_sym_names += ['squeezenet0_flatten0_flatten0']
+            if exclude_first_conv:
+                excluded_sym_names += ['squeezenet0_conv0_fwd']
+        elif args.model.find('mobilenet') != -1 and args.model.find('v2') == -1:
+            excluded_sym_names += ['mobilenet0_flatten0_flatten0',
+                                'mobilenet0_pool0_fwd']
+            if exclude_first_conv:
+                excluded_sym_names += ['mobilenet0_conv0_fwd']
+        elif args.model.find('mobilenet') != -1 and args.model.find('v2') != -1:
+            excluded_sym_names += ['mobilenetv20_output_flatten0_flatten0']
+            if exclude_first_conv:
+                excluded_sym_names += ['mobilenetv20_conv0_fwd']
+        elif args.model == 'inceptionv3':
+            if exclude_first_conv:
+                excluded_sym_names += ['inception30_conv0_fwd']
+        else:
+            raise ValueError('Currently, model %s is not supported in this script' % args.model)
+    else:
+        logger.info('Please set proper RGB configs for model %s' % args.model)
         # add rgb mean/std of your model.
         rgb_mean = '0,0,0'
         rgb_std = '0,0,0'
         # add layer names you donnot want to quantize.
+        logger.info('Please set proper excluded_sym_names for model %s' % args.model)
         excluded_sym_names += ['layers']
         if exclude_first_conv:
             excluded_sym_names += ['layers']
-    else:
-        raise ValueError('model %s is not supported in this script' % args.model)
+
+    logger.info('These layers have been excluded %s' % excluded_sym_names)
 
     label_name = args.label_name
     logger.info('label_name = %s' % label_name)
@@ -281,10 +270,10 @@ if __name__ == '__main__':
     combine_mean_std.update(std_args)
     if calib_mode == 'none':
         logger.info('Quantizing FP32 model %s' % args.model)
-        qsym, qarg_params, aux_params = quantize_model(sym=sym, arg_params=arg_params, aux_params=aux_params,
-                                                       ctx=ctx, excluded_sym_names=excluded_sym_names,
-                                                       calib_mode=calib_mode, quantized_dtype=args.quantized_dtype,
-                                                       logger=logger)
+        qsym, qarg_params, aux_params = quantize_model_mkldnn(sym=sym, arg_params=arg_params, aux_params=aux_params,
+                                                              ctx=ctx, excluded_sym_names=excluded_sym_names,
+                                                              calib_mode=calib_mode, quantized_dtype=args.quantized_dtype,
+                                                              logger=logger)
         sym_name = '%s-symbol.json' % (prefix + '-quantized')
     else:
         logger.info('Creating ImageRecordIter for reading calibration dataset')
@@ -301,12 +290,12 @@ if __name__ == '__main__':
                                      seed=args.shuffle_seed,
                                      **combine_mean_std)
 
-        qsym, qarg_params, aux_params = quantize_model(sym=sym, arg_params=arg_params, aux_params=aux_params,
-                                                        ctx=ctx, excluded_sym_names=excluded_sym_names,
-                                                        calib_mode=calib_mode, calib_data=data,
-                                                        num_calib_examples=num_calib_batches * batch_size,
-                                                        calib_layer=calib_layer, quantized_dtype=args.quantized_dtype,
-                                                        label_names=(label_name,), logger=logger)
+        qsym, qarg_params, aux_params = quantize_model_mkldnn(sym=sym, arg_params=arg_params, aux_params=aux_params,
+                                                              ctx=ctx, excluded_sym_names=excluded_sym_names,
+                                                              calib_mode=calib_mode, calib_data=data,
+                                                              num_calib_examples=num_calib_batches * batch_size,
+                                                              calib_layer=calib_layer, quantized_dtype=args.quantized_dtype,
+                                                              label_names=(label_name,), logger=logger)
         if calib_mode == 'entropy':
             suffix = '-quantized-%dbatches-entropy' % num_calib_batches
         elif calib_mode == 'naive':
@@ -315,7 +304,6 @@ if __name__ == '__main__':
             raise ValueError('unknow calibration mode %s received, only supports `none`, `naive`, and `entropy`'
                              % calib_mode)
         sym_name = '%s-symbol.json' % (prefix + suffix)
-    qsym = qsym.get_backend_symbol('MKLDNN_QUANTIZE')
     save_symbol(sym_name, qsym, logger)
     param_name = '%s-%04d.params' % (prefix + '-quantized', epoch)
     save_params(param_name, qarg_params, aux_params, logger)
diff --git a/include/mxnet/c_api.h b/include/mxnet/c_api.h
index bd30e44f9..058f859ae 100644
--- a/include/mxnet/c_api.h
+++ b/include/mxnet/c_api.h
@@ -506,6 +506,15 @@ MXNET_DLL int MXGetGPUMemoryInformation64(int dev, uint64_t *free_mem, uint64_t
  */
 MXNET_DLL int MXGetVersion(int *out);
 
+/*!
+ * \brief Load TVM operator from the binary library
+ * \param libpath TVM operators lib file
+ * \return 0 when success, -1 when failure happens
+ */
+#if MXNET_USE_TVM_OP
+MXNET_DLL int MXLoadTVMOp(const char *libpath);
+#endif  // MXNET_USE_TVM_OP
+
 
 //-------------------------------------
 // Part 1: NDArray creation and deletion
diff --git a/include/mxnet/ndarray.h b/include/mxnet/ndarray.h
index 428245b56..176aa0aaa 100644
--- a/include/mxnet/ndarray.h
+++ b/include/mxnet/ndarray.h
@@ -190,11 +190,7 @@ class NDArray {
   /*!
    * \brief set the correct shape of NDArray directly from the storage_shape of its own chunk.
    */
-  void SetShapeFromChunk() {
-    if (!(ptr_->storage_shape.ndim() == 1 && ptr_->storage_shape[0] == 0)) {
-      shape_ = ptr_->storage_shape;
-    }
-  }
+  void SetShapeFromChunk();
   /*
    * This indicates whether an array is a view of another array (created by
    * reshape or slice). If an array is a view and the data is stored in
diff --git a/make/config.mk b/make/config.mk
index a333db5d7..be776f349 100644
--- a/make/config.mk
+++ b/make/config.mk
@@ -84,7 +84,7 @@ endif
 ENABLE_CUDA_RTC = 0
 
 # whether use CuDNN R3 library
-USE_CUDNN = 0
+USE_CUDNN = 1
 
 # whether to use NVTX when profiling
 USE_NVTX = 0
diff --git a/python/mxnet/autograd.py b/python/mxnet/autograd.py
index f461b77e2..6f1cc4367 100644
--- a/python/mxnet/autograd.py
+++ b/python/mxnet/autograd.py
@@ -197,6 +197,9 @@ def predict_mode():
 def mark_variables(variables, gradients, grad_reqs='write'):
     """Mark NDArrays as variables to compute gradient for autograd.
 
+    This is equivalent to the function .attach_grad() in a variable, but with this
+    call we can set the gradient to any value.
+
     Parameters
     ----------
     variables: NDArray or list of NDArray
diff --git a/python/mxnet/contrib/quantization.py b/python/mxnet/contrib/quantization.py
index b94b5a8da..fa2ab1842 100644
--- a/python/mxnet/contrib/quantization.py
+++ b/python/mxnet/contrib/quantization.py
@@ -543,3 +543,240 @@ def quantize_model(sym, arg_params, aux_params,
     qarg_params = _quantize_params(qsym, arg_params, th_dict)
 
     return qsym, qarg_params, aux_params
+
+def quantize_model_mkldnn(sym, arg_params, aux_params,
+                          data_names=('data',), label_names=('softmax_label',),
+                          ctx=cpu(), excluded_sym_names=None, calib_mode='entropy',
+                          calib_data=None, num_calib_examples=None, calib_layer=None,
+                          quantized_dtype='int8', logger=logging):
+    """User-level API for generating a fusion + quantized model from a FP32 model
+    w/ or w/o calibration with Intel MKL-DNN.
+    The backend quantized operators are only enabled for Linux systems. Please do not run
+    inference using the quantized models on Windows for now.
+
+    Parameters
+    ----------
+    sym : str or Symbol
+        Defines the structure of a neural network for FP32 data types.
+    arg_params : dict
+        Dictionary of name to `NDArray`.
+    aux_params : dict
+        Dictionary of name to `NDArray`.
+    data_names : a list of strs
+        Data names required for creating a Module object to run forward propagation on the
+        calibration dataset.
+    label_names : a list of strs
+        Label names required for creating a Module object to run forward propagation on the
+        calibration dataset.
+    ctx : Context
+        Defines the device that users want to run forward propagation on the calibration
+        dataset for collecting layer output statistics. Currently, only supports single context.
+    excluded_sym_names : list of strings
+        A list of strings representing the names of the symbols that users want to excluding
+        from being quantized.
+    calib_mode : str
+        If calib_mode='none', no calibration will be used and the thresholds for
+        requantization after the corresponding layers will be calculated at runtime by
+        calling min and max operators. The quantized models generated in this
+        mode are normally 10-20% slower than those with calibrations during inference.
+        If calib_mode='naive', the min and max values of the layer outputs from a calibration
+        dataset will be directly taken as the thresholds for quantization.
+        If calib_mode='entropy' (default mode), the thresholds for quantization will be
+        derived such that the KL divergence between the distributions of FP32 layer outputs and
+        quantized layer outputs is minimized based upon the calibration dataset.
+    calib_data : DataIter
+        A data iterator initialized by the calibration dataset.
+    num_calib_examples : int or None
+        The maximum number of examples that user would like to use for calibration. If not provided,
+        the whole calibration dataset will be used.
+    calib_layer : function
+        Given a layer's output name in string, return True or False for deciding whether to
+        calibrate this layer. If yes, the statistics of the layer's output will be collected;
+        otherwise, no information of the layer's output will be collected. If not provided,
+        all the layers' outputs that need requantization will be collected.
+    quantized_dtype : str
+        The quantized destination type for input data. Currently support 'int8'
+        , 'uint8' and 'auto'. 'auto' means automatically select output type according to calibration result.
+        Default value is 'int8'.
+    logger : Object
+        A logging object for printing information during the process of quantization.
+
+    Returns
+    -------
+    tuple
+        A tuple of quantized symbol, quantized arg_params, and aux_params.
+    -------
+    """
+    if ctx != cpu():
+        raise ValueError(
+            'quantize_model_mkldnn only support Intel cpu platform with MKL-DNN Backend')
+
+    sym = sym.get_backend_symbol('MKLDNN_QUANTIZE')
+
+    qsym, qarg_params, aux_params = quantize_model(sym=sym, arg_params=arg_params, aux_params=aux_params,
+                                                   data_names=data_names, label_names=label_names,
+                                                   ctx=ctx, excluded_sym_names=excluded_sym_names,
+                                                   calib_mode=calib_mode, calib_data=calib_data,
+                                                   num_calib_examples=num_calib_examples, calib_layer=calib_layer,
+                                                   quantized_dtype=quantized_dtype, logger=logger)
+
+    qsym = qsym.get_backend_symbol('MKLDNN_QUANTIZE')
+
+    return qsym, qarg_params, aux_params
+
+def quantize_graph(sym, arg_params, aux_params,
+                   excluded_sym_names=None, calib_mode='entropy',
+                   calib_layer=None, quantized_dtype='int8', logger=logging):
+    """User-level API for generating a quantized model from a FP32 model w/o calibration
+    and a collector for naive or entropy calibration.
+    The backend quantized operators are only enabled for Linux systems. Please do not run
+    inference using the quantized models on Windows for now.
+    The quantization implementation adopts the TensorFlow's approach:
+    https://www.tensorflow.org/performance/quantization.
+    The calibration implementation borrows the idea of Nvidia's 8-bit Inference with TensorRT:
+    http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf
+    and adapts the method to MXNet.
+    Parameters
+    ----------
+    sym : str or Symbol
+        Defines the structure of a neural network for FP32 data types.
+    arg_params : dict
+        Dictionary of name to `NDArray`.
+    aux_params : dict
+        Dictionary of name to `NDArray`.
+    excluded_sym_names : list of strings
+        A list of strings representing the names of the symbols that users want to excluding
+        from being quantized.
+    calib_mode : str
+        If calib_mode='none', no calibration will be used and the thresholds for
+        requantization after the corresponding layers will be calculated at runtime by
+        calling min and max operators. The quantized models generated in this
+        mode are normally 10-20% slower than those with calibrations during inference.
+        If calib_mode='naive', the min and max values of the layer outputs from a calibration
+        dataset will be directly taken as the thresholds for quantization.
+        If calib_mode='entropy' (default mode), the thresholds for quantization will be
+        derived such that the KL divergence between the distributions of FP32 layer outputs and
+        quantized layer outputs is minimized based upon the calibration dataset.
+    calib_layer : function
+        Given a layer's output name in string, return True or False for deciding whether to
+        calibrate this layer. If yes, the statistics of the layer's output will be collected;
+        otherwise, no information of the layer's output will be collected. If not provided,
+        all the layers' outputs that need requantization will be collected.
+    quantized_dtype : str
+        The quantized destination type for input data. Currently support 'int8'
+        , 'uint8' and 'auto'. 'auto' means automatically select output type according to calibration result.
+        Default value is 'int8'.
+    logger : Object
+        A logging object for printing information during the process of quantization.
+    Returns
+    -------
+    tuple
+        A tuple of quantized symbol, quantized arg_params, aux_params and collector.
+    -------
+    """
+    if excluded_sym_names is None:
+        excluded_sym_names = []
+    if not isinstance(excluded_sym_names, list):
+        raise ValueError('excluded_sym_names must be a list of strings representing'
+                         ' the names of the symbols that will not be quantized,'
+                         ' while received type %s' % str(type(excluded_sym_names)))
+
+    logger.info('Quantizing graph')
+    if quantized_dtype not in ('int8', 'uint8', 'auto'):
+        raise ValueError('unknown quantized_dtype %s received,'
+                         ' expected `int8`, `uint8` or `auto`' % quantized_dtype)
+    qsym = _quantize_symbol(sym, excluded_symbols=excluded_sym_names,
+                            offline_params=list(arg_params.keys()),
+                            quantized_dtype=quantized_dtype)
+
+    th_dict = {}
+    collector = None
+    if calib_mode is not None and calib_mode != 'none':
+        if calib_mode == 'entropy':
+            collector = _LayerOutputCollector(
+                include_layer=calib_layer, logger=logger)
+            logger.info(
+                'Create a layer output collector for entropy calibration.')
+        elif calib_mode == 'naive':
+            collector = _LayerOutputMinMaxCollector(
+                include_layer=calib_layer, logger=logger)
+            logger.info(
+                'Create a layer output minmax collector for naive calibration')
+        else:
+            raise ValueError('unknown calibration mode %s received,'
+                             ' expected `none`, `naive`, or `entropy`' % calib_mode)
+        logger.info('Collector created, please use set_monitor_callback'
+                    ' to collect calibration information.')
+
+    logger.info('Quantizing parameters')
+    qarg_params = _quantize_params(qsym, arg_params, th_dict)
+
+    return qsym, qarg_params, aux_params, collector
+
+def calib_graph(qsym, arg_params, aux_params, collector,
+                calib_mode='entropy', quantized_dtype='int8', logger=logging):
+    """User-level API for calibrating a quantized model using a filled collector.
+    The backend quantized operators are only enabled for Linux systems. Please do not run
+    inference using the quantized models on Windows for now.
+    The quantization implementation adopts the TensorFlow's approach:
+    https://www.tensorflow.org/performance/quantization.
+    The calibration implementation borrows the idea of Nvidia's 8-bit Inference with TensorRT:
+    http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf
+    and adapts the method to MXNet.
+    Parameters
+    ----------
+    qsym : str or Symbol
+        Defines the structure of a neural network for INT8 data types.
+    arg_params : dict
+        Dictionary of name to `NDArray`.
+    aux_params : dict
+        Dictionary of name to `NDArray`.
+    collector : function
+        layer collector for naive or entropy calibration.
+    calib_mode : str
+        If calib_mode='none', no calibration will be used and the thresholds for
+        requantization after the corresponding layers will be calculated at runtime by
+        calling min and max operators. The quantized models generated in this
+        mode are normally 10-20% slower than those with calibrations during inference.
+        If calib_mode='naive', the min and max values of the layer outputs from a calibration
+        dataset will be directly taken as the thresholds for quantization.
+        If calib_mode='entropy' (default mode), the thresholds for quantization will be
+        derived such that the KL divergence between the distributions of FP32 layer outputs and
+        quantized layer outputs is minimized based upon the calibration dataset.
+    calib_layer : function
+        Given a layer's output name in string, return True or False for deciding whether to
+        calibrate this layer. If yes, the statistics of the layer's output will be collected;
+        otherwise, no information of the layer's output will be collected. If not provided,
+        all the layers' outputs that need requantization will be collected.
+    quantized_dtype : str
+        The quantized destination type for input data. Currently support 'int8'
+        , 'uint8' and 'auto'. 'auto' means automatically select output type according to calibration result.
+        Default value is 'int8'.
+    logger : Object
+        A logging object for printing information during the process of quantization.
+    Returns
+    -------
+    tuple
+        A tuple of calibrated symbol, quantized arg_params, aux_params.
+    -------
+    """
+    th_dict = {}
+    if calib_mode is not None and calib_mode != 'none':
+        if calib_mode == 'entropy':
+            logger.info('Calculating optimal thresholds for quantization')
+            th_dict = _get_optimal_thresholds(
+                collector.nd_dict, quantized_dtype, logger=logger)
+        elif calib_mode == 'naive':
+            th_dict = collector.min_max_dict
+        else:
+            raise ValueError('unknown calibration mode %s received,'
+                             ' expected `none`, `naive`, or `entropy`' % calib_mode)
+        logger.info('Calibrating quantized symbol')
+        qsym = _calibrate_quantized_sym(qsym, th_dict)
+    else:
+        raise ValueError('please set calibration mode to naive or entropy.')
+
+    logger.info('Quantizing parameters')
+    qarg_params = _quantize_params(qsym, arg_params, th_dict)
+
+    return qsym, qarg_params, aux_params
diff --git a/python/mxnet/gluon/contrib/__init__.py b/python/mxnet/gluon/contrib/__init__.py
index 83be8a39b..7590eb740 100644
--- a/python/mxnet/gluon/contrib/__init__.py
+++ b/python/mxnet/gluon/contrib/__init__.py
@@ -25,3 +25,5 @@ from . import rnn
 from . import cnn
 
 from . import data
+
+from . import estimator
diff --git a/python/mxnet/gluon/contrib/estimator/__init__.py b/python/mxnet/gluon/contrib/estimator/__init__.py
index 58600dadf..bb0a0917c 100644
--- a/python/mxnet/gluon/contrib/estimator/__init__.py
+++ b/python/mxnet/gluon/contrib/estimator/__init__.py
@@ -17,5 +17,7 @@
 
 # pylint: disable=wildcard-import
 """Gluon Estimator Module"""
+from . import estimator
+from . import event_handler
 from .estimator import *
 from .event_handler import *
diff --git a/python/mxnet/gluon/contrib/estimator/estimator.py b/python/mxnet/gluon/contrib/estimator/estimator.py
index da1a3915c..b6142e100 100644
--- a/python/mxnet/gluon/contrib/estimator/estimator.py
+++ b/python/mxnet/gluon/contrib/estimator/estimator.py
@@ -24,9 +24,15 @@ import warnings
 
 from .event_handler import MetricHandler, ValidationHandler, LoggingHandler, StoppingHandler
 from .event_handler import TrainBegin, EpochBegin, BatchBegin, BatchEnd, EpochEnd, TrainEnd
-from .... import gluon, autograd
+from ...data import DataLoader
+from ...loss import SoftmaxCrossEntropyLoss
+from ...loss import Loss as gluon_loss
+from ...trainer import Trainer
+from ...utils import split_and_load
+from .... import autograd
 from ....context import Context, cpu, gpu, num_gpus
-from ....metric import EvalMetric, Loss, Accuracy
+from ....metric import EvalMetric, Accuracy
+from ....metric import Loss as metric_loss
 
 __all__ = ['Estimator']
 
@@ -69,9 +75,9 @@ class Estimator(object):
         self.trainer = self._check_trainer(trainer)
 
     def _check_loss(self, loss):
-        if isinstance(loss, gluon.loss.Loss):
+        if isinstance(loss, gluon_loss):
             loss = [loss]
-        elif isinstance(loss, list) and all([isinstance(l, gluon.loss.Loss) for l in loss]):
+        elif isinstance(loss, list) and all([isinstance(l, gluon_loss) for l in loss]):
             loss = loss
         else:
             raise ValueError("loss must be a Loss or a list of Loss, "
@@ -146,9 +152,9 @@ class Estimator(object):
         if not trainer:
             warnings.warn("No trainer specified, default SGD optimizer "
                           "with learning rate 0.001 is used.")
-            trainer = gluon.Trainer(self.net.collect_params(),
-                                    'sgd', {'learning_rate': 0.001})
-        elif not isinstance(trainer, gluon.Trainer):
+            trainer = Trainer(self.net.collect_params(),
+                              'sgd', {'learning_rate': 0.001})
+        elif not isinstance(trainer, Trainer):
             raise ValueError("Trainer must be a Gluon Trainer instance, refer to "
                              "gluon.Trainer:{}".format(trainer))
         return trainer
@@ -165,8 +171,8 @@ class Estimator(object):
     def _get_data_and_label(self, batch, ctx, batch_axis=0):
         data = batch[0]
         label = batch[1]
-        data = gluon.utils.split_and_load(data, ctx_list=ctx, batch_axis=batch_axis)
-        label = gluon.utils.split_and_load(label, ctx_list=ctx, batch_axis=batch_axis)
+        data = split_and_load(data, ctx_list=ctx, batch_axis=batch_axis)
+        label = split_and_load(label, ctx_list=ctx, batch_axis=batch_axis)
         return data, label
 
     def prepare_loss_and_metrics(self):
@@ -179,13 +185,13 @@ class Estimator(object):
         """
         if any(not hasattr(self, attribute) for attribute in
                ['train_metrics', 'val_metrics']):
-            # Use default mx.metric.Accuracy() for gluon.loss.SoftmaxCrossEntropyLoss()
-            if not self.train_metrics and any([isinstance(l, gluon.loss.SoftmaxCrossEntropyLoss) for l in self.loss]):
+            # Use default mx.metric.Accuracy() for SoftmaxCrossEntropyLoss()
+            if not self.train_metrics and any([isinstance(l, SoftmaxCrossEntropyLoss) for l in self.loss]):
                 self.train_metrics = [Accuracy()]
             self.val_metrics = []
             for loss in self.loss:
                 # remove trailing numbers from loss name to avoid confusion
-                self.train_metrics.append(Loss(loss.name.rstrip('1234567890')))
+                self.train_metrics.append(metric_loss(loss.name.rstrip('1234567890')))
             for metric in self.train_metrics:
                 val_metric = copy.deepcopy(metric)
                 metric.name = "train " + metric.name
@@ -208,10 +214,10 @@ class Estimator(object):
          batch_axis : int, default 0
              Batch axis to split the validation data into devices.
          """
-        if not isinstance(val_data, gluon.data.DataLoader):
+        if not isinstance(val_data, DataLoader):
             raise ValueError("Estimator only support input as Gluon DataLoader. Alternatively, you "
                              "can transform your DataIter or any NDArray into Gluon DataLoader. "
-                             "Refer to gluon.data.dataloader")
+                             "Refer to gluon.data.DataLoader")
 
         for metric in val_metrics:
             metric.reset()
@@ -222,7 +228,7 @@ class Estimator(object):
             loss = [self.loss[0](y_hat, y) for y_hat, y in zip(pred, label)]
             # update metrics
             for metric in val_metrics:
-                if isinstance(metric, Loss):
+                if isinstance(metric, metric_loss):
                     metric.update(0, loss)
                 else:
                     metric.update(label, pred)
@@ -254,7 +260,7 @@ class Estimator(object):
         batch_axis : int, default 0
             Batch axis to split the training data into devices.
         """
-        if not isinstance(train_data, gluon.data.DataLoader):
+        if not isinstance(train_data, DataLoader):
             raise ValueError("Estimator only support input as Gluon DataLoader. Alternatively, you "
                              "can transform your DataIter or any NDArray into Gluon DataLoader. "
                              "Refer to gluon.data.dataloader")
@@ -328,28 +334,36 @@ class Estimator(object):
     def _prepare_default_handlers(self, val_data, event_handlers):
         event_handlers = event_handlers or []
         default_handlers = []
-        train_metrics, val_metrics = self.prepare_loss_and_metrics()
+        self.prepare_loss_and_metrics()
 
         # no need to add to default handler check as StoppingHandler does not use metrics
         event_handlers.append(StoppingHandler(self.max_epoch, self.max_batch))
+        default_handlers.append("StoppingHandler")
 
         if not any(isinstance(handler, MetricHandler) for handler in event_handlers):
-            event_handlers.append(MetricHandler(train_metrics=train_metrics))
+            event_handlers.append(MetricHandler(train_metrics=self.train_metrics))
             default_handlers.append("MetricHandler")
 
-        if val_data and not any(isinstance(handler, ValidationHandler) for handler in event_handlers):
-            event_handlers.append(ValidationHandler(val_data=val_data, eval_fn=self.evaluate,
-                                                    val_metrics=val_metrics))
-            default_handlers.append("ValidationHandler")
+        if not any(isinstance(handler, ValidationHandler) for handler in event_handlers):
+            # no validation handler
+            if val_data:
+                # add default validation handler if validation data found
+                event_handlers.append(ValidationHandler(val_data=val_data, eval_fn=self.evaluate,
+                                                        val_metrics=self.val_metrics))
+                default_handlers.append("ValidationHandler")
+                val_metrics = self.val_metrics
+            else:
+                # set validation metrics to None if no validation data and no validation handler
+                val_metrics = []
 
         if not any(isinstance(handler, LoggingHandler) for handler in event_handlers):
-            event_handlers.append(LoggingHandler(train_metrics=train_metrics,
+            event_handlers.append(LoggingHandler(train_metrics=self.train_metrics,
                                                  val_metrics=val_metrics))
             default_handlers.append("LoggingHandler")
 
         # if there is a mix of user defined event handlers and default event handlers
         # they should have the same set of loss and metrics
-        if default_handlers:
+        if default_handlers and len(event_handlers) != len(default_handlers):
             msg = "You are training with the following default event handlers: %s. " \
                   "They use loss and metrics from estimator.prepare_loss_and_metrics(). " \
                   "Please use the same set of metrics for all your other handlers." % \
@@ -368,7 +382,7 @@ class Estimator(object):
             # remove None metric references
             references = set([ref for ref in references if ref])
             for metric in references:
-                if metric not in train_metrics + val_metrics:
+                if metric not in self.train_metrics + self.val_metrics:
                     msg = "We have added following default handlers for you: %s and used " \
                           "estimator.prepare_loss_and_metrics() to pass metrics to " \
                           "those handlers. Please use the same set of metrics " \
diff --git a/python/mxnet/gluon/contrib/estimator/event_handler.py b/python/mxnet/gluon/contrib/estimator/event_handler.py
index ed97c7bc3..da2c84455 100644
--- a/python/mxnet/gluon/contrib/estimator/event_handler.py
+++ b/python/mxnet/gluon/contrib/estimator/event_handler.py
@@ -26,7 +26,12 @@ import warnings
 
 import numpy as np
 
-from ....metric import EvalMetric, Loss
+from ....metric import EvalMetric
+from ....metric import Loss as metric_loss
+
+__all__ = ['TrainBegin', 'TrainEnd', 'EpochBegin', 'EpochEnd', 'BatchBegin', 'BatchEnd',
+           'StoppingHandler', 'MetricHandler', 'ValidationHandler',
+           'LoggingHandler', 'CheckpointHandler', 'EarlyStoppingHandler']
 
 
 class TrainBegin(object):
@@ -127,7 +132,7 @@ class MetricHandler(EpochBegin, BatchEnd):
         label = kwargs['label']
         loss = kwargs['loss']
         for metric in self.train_metrics:
-            if isinstance(metric, Loss):
+            if isinstance(metric, metric_loss):
                 # metric wrapper for loss values
                 metric.update(0, loss)
             else:
@@ -135,7 +140,7 @@ class MetricHandler(EpochBegin, BatchEnd):
 
 
 class ValidationHandler(TrainBegin, BatchEnd, EpochEnd):
-    """"Validation Handler that evaluate model on validation dataset
+    """Validation Handler that evaluate model on validation dataset
 
     :py:class:`ValidationHandler` takes validation dataset, an evaluation function,
     metrics to be evaluated, and how often to run the validation. You can provide custom
@@ -430,7 +435,7 @@ class CheckpointHandler(TrainBegin, BatchEnd, EpochEnd):
         self.current_epoch = 0
         self.current_batch = 0
         if self.save_best:
-            self.best = np.Inf if self.monitor_op == np.less else -np.Inf # pylint: disable=comparison-with-callable
+            self.best = np.Inf if self.monitor_op == np.less else -np.Inf  # pylint: disable=comparison-with-callable
         if self.resume_from_checkpoint:
             error_msg = "To use resume from checkpoint, you must only specify " \
                         "the same type of period you used for training." \
@@ -506,12 +511,12 @@ class CheckpointHandler(TrainBegin, BatchEnd, EpochEnd):
 
     def _save_symbol(self, estimator):
         symbol_file = os.path.join(self.model_dir, self.model_prefix + '-symbol.json')
-        if hasattr(estimator.net, '_cached_graph'):
+        if hasattr(estimator.net, '_cached_graph') and estimator.net._cached_graph:
             sym = estimator.net._cached_graph[1]
             sym.save(symbol_file)
         else:
-            self.logger.info("Model architecture(symbol file) is not saved, please use HybridBlock"
-                             "to construct your model, can call net.hybridize() before passing to"
+            self.logger.info("Model architecture(symbol file) is not saved, please use HybridBlock "
+                             "to construct your model, can call net.hybridize() before passing to "
                              "Estimator in order to save model architecture as %s.", symbol_file)
 
     def _save_params_and_trainer(self, estimator, file_prefix):
@@ -666,7 +671,7 @@ class EarlyStoppingHandler(TrainBegin, EpochEnd, TrainEnd):
                                  "if you want otherwise", self.monitor.get()[0])
                 self.monitor_op = np.less
 
-        if self.monitor_op == np.greater: # pylint: disable=comparison-with-callable
+        if self.monitor_op == np.greater:  # pylint: disable=comparison-with-callable
             self.min_delta *= 1
         else:
             self.min_delta *= -1
@@ -679,7 +684,7 @@ class EarlyStoppingHandler(TrainBegin, EpochEnd, TrainEnd):
         if self.baseline is not None:
             self.best = self.baseline
         else:
-            self.best = np.Inf if self.monitor_op == np.less else -np.Inf # pylint: disable=comparison-with-callable
+            self.best = np.Inf if self.monitor_op == np.less else -np.Inf  # pylint: disable=comparison-with-callable
 
     def epoch_end(self, estimator, *args, **kwargs):
         monitor_name, monitor_value = self.monitor.get()
diff --git a/python/mxnet/ndarray/ndarray.py b/python/mxnet/ndarray/ndarray.py
index 3fb1af6a7..0b7dca4eb 100644
--- a/python/mxnet/ndarray/ndarray.py
+++ b/python/mxnet/ndarray/ndarray.py
@@ -205,6 +205,10 @@ fixed-size items.
             self.handle, ctypes.byref(shared_pid), ctypes.byref(shared_id)))
         return shared_pid.value, shared_id.value, self.shape, self.dtype
 
+    def __abs__(self):
+        """x.__abs__() <=> abs(x) <=> x.abs() <=> mx.nd.abs(x, y)"""
+        return self.abs()
+
     def __add__(self, other):
         """x.__add__(y) <=> x+y <=> mx.nd.add(x, y) """
         return add(self, other)
@@ -1259,13 +1263,38 @@ fixed-size items.
         """
         return op.sign(self, *args, **kwargs)
 
-    def flatten(self, *args, **kwargs):
-        """Convenience fluent method for :py:func:`flatten`.
+    def flatten(self, inplace=False):
+        """Flatten this array without altering any data.
 
-        The arguments are the same as for :py:func:`flatten`, with
-        this array as data.
+        Parameters
+        ----------
+        inplace : bool, default False
+            If True, this method returns a **view** of this array
+            that shares data with this array. Otherwise, a copy is returned.
+
+        Returns
+        -------
+        NDArray
+            An array with flattened shape `(d1, d2*...*dk)` that shares data with
+            this array with shape `(d1, d2, ..., dk)`.
+
+        Examples
+        --------
+        >>> x = mx.nd.arange(30).reshape(5,2,3)
+        >>> y = x.flatten(inplace=True)
+        >>> z = x.flatten()
+        >>> y.shape
+        (5, 6)
+        >>> y[0].asnumpy()
+        array([0., 1., 2., 3., 4., 5.], dtype=float32)
+        >>> y[:] = -1
+        >>> x[0].asnumpy()
+        array([[-1., -1., -1.],
+               [-1., -1., -1.]], dtype=float32)
+        >>> z[0].asnumpy()
+        array([0., 1., 2., 3., 4., 5.], dtype=float32)
         """
-        return op.flatten(self, *args, **kwargs)
+        return op.flatten(self) if not inplace else self.reshape((0, -1))
 
     def shape_array(self, *args, **kwargs):
         """Convenience fluent method for :py:func:`shape_array`.
@@ -1283,13 +1312,52 @@ fixed-size items.
         """
         return op.size_array(self, *args, **kwargs)
 
-    def expand_dims(self, *args, **kwargs):
-        """Convenience fluent method for :py:func:`expand_dims`.
+    def expand_dims(self, axis, inplace=False):
+        """Adds an additional dimension to the current array without altering any data.
 
-        The arguments are the same as for :py:func:`expand_dims`, with
-        this array as data.
+        Parameters
+        ----------
+        axis : int
+            Position where new axis is to be inserted.
+            Suppose that the input NDArray's dimension is ndim,
+            the range of the inserted axis is [-ndim, ndim].
+        inplace : bool, default False
+            If True, this method returns a **view** of this array
+            that shares data with this array. Otherwise, a copy is returned.
+
+        Returns
+        -------
+        NDArray
+            An array with expanded shape `(d1, d2, ..., 1, di, ..., dk)`
+            that shares data with this array with shape `(d1, d2, ..., dk)`,
+            given input axis `i`.
+
+        Examples
+        --------
+        >>> x = mx.nd.arange(6).reshape(2,3)
+        >>> y = x.expand_dims(1, inplace=True)
+        >>> z = x.expand_dims(1)
+        >>> y.shape
+        (2, 1, 3)
+        >>> y[0].asnumpy()
+        array([[0., 1., 2.]], dtype=float32)
+        >>> y[:] = -1
+        >>> x.asnumpy()
+        array([[-1., -1., -1.],
+               [-1., -1., -1.]], dtype=float32)
+        >>> z[0].asnumpy()
+        array([[0., 1., 2.]], dtype=float32)
         """
-        return op.expand_dims(self, *args, **kwargs)
+        if not inplace:
+            return op.expand_dims(self, axis=axis)
+        else:
+            new_shape = list(self.shape)
+            assert -len(new_shape)-1 <= axis <= len(new_shape), \
+                    "axis {} is out of range for {}d array".format(axis, len(new_shape))
+            if axis < 0:
+                axis += len(new_shape) + 1
+            new_shape.insert(axis, 1)
+            return self.reshape(new_shape)
 
     def tile(self, *args, **kwargs):
         """Convenience fluent method for :py:func:`tile`.
@@ -1699,13 +1767,45 @@ fixed-size items.
         """
         return op.softmin(self, *args, **kwargs)
 
-    def squeeze(self, *args, **kwargs):
-        """Convenience fluent method for :py:func:`squeeze`.
+    def squeeze(self, axis=None, inplace=False):
+        """Remove dimensions with size 1 from this array without altering any data.
 
-        The arguments are the same as for :py:func:`squeeze`, with
-        this array as data.
+        Parameters
+        ----------
+        axis : int, tuple of int, or None
+            Selects a subset of the single-dimensional entries in the shape.
+            If an axis is selected with shape entry greater than one, an error is raised.
+        inplace : bool, default False
+            If True, this method returns a **view** of this array
+            that shares data with this array. Otherwise, a copy is returned.
         """
-        return op.squeeze(self, *args, **kwargs)
+        if not inplace:
+            return op.squeeze(self, axis=axis)
+        else:
+            new_shape = list(self.shape)
+            axes = axis # rename variable for readability
+            if isinstance(axes, int):
+                axes = [axes]
+            if axes:
+                assert len(axes) == len(set(axes)), \
+                    "axis {} contains duplicate which is not allowed.".format(axes)
+                resolved_axes = [i if i >= 0 else i+len(self.shape) for i in axes]
+                for arg_axis, actual_axis in zip(axes, resolved_axes):
+                    assert -len(new_shape) <= arg_axis < len(new_shape), \
+                        "axis {} is out of range for {}d array".format(arg_axis, len(new_shape))
+                    axis_size = new_shape[actual_axis]
+                    assert axis_size == 1, \
+                        "Squeeze target axis {} must be size 1, got {}.".format(arg_axis, axis_size)
+                for i in sorted(resolved_axes, reverse=True):
+                    del new_shape[i]
+            else:
+                for i in reversed(range(len(new_shape))):
+                    if new_shape[i] == 1:
+                        del new_shape[i]
+            if not new_shape:
+                new_shape.append(1)
+
+            return self.reshape(new_shape)
 
     # pylint: disable= undefined-variable
     def broadcast_to(self, shape):
@@ -2147,6 +2247,8 @@ fixed-size items.
         """Attach a gradient buffer to this NDArray, so that `backward`
         can compute gradient with respect to it.
 
+        The gradient is initialized to zeros.
+
         Parameters
         ----------
         grad_req : {'write', 'add', 'null'}
diff --git a/python/mxnet/symbol/symbol.py b/python/mxnet/symbol/symbol.py
index d3cd519b9..68322297c 100644
--- a/python/mxnet/symbol/symbol.py
+++ b/python/mxnet/symbol/symbol.py
@@ -93,6 +93,10 @@ class Symbol(SymbolBase):
         """
         return (self[i] for i in range(len(self)))
 
+    def __abs__(self):
+        """x.__abs__() <=> abs(x) <=> x.abs() <=> mx.symbol.abs(x, y)"""
+        return self.abs()
+
     def __add__(self, other):
         """x.__add__(y) <=> x+y
 
@@ -2083,13 +2087,13 @@ class Symbol(SymbolBase):
         """
         return op.sign(self, *args, **kwargs)
 
-    def flatten(self, *args, **kwargs):
+    def flatten(self, inplace=False, **kwargs): # pylint: disable=unused-argument
         """Convenience fluent method for :py:func:`flatten`.
 
         The arguments are the same as for :py:func:`flatten`, with
         this array as data.
         """
-        return op.flatten(self, *args, **kwargs)
+        return op.flatten(self, **kwargs)
 
     def shape_array(self, *args, **kwargs):
         """Convenience fluent method for :py:func:`shape_array`.
@@ -2107,13 +2111,13 @@ class Symbol(SymbolBase):
         """
         return op.size_array(self, *args, **kwargs)
 
-    def expand_dims(self, *args, **kwargs):
+    def expand_dims(self, axis, inplace=False, **kwargs): # pylint: disable=unused-argument
         """Convenience fluent method for :py:func:`expand_dims`.
 
         The arguments are the same as for :py:func:`expand_dims`, with
         this array as data.
         """
-        return op.expand_dims(self, *args, **kwargs)
+        return op.expand_dims(self, axis=axis, **kwargs)
 
     def broadcast_to(self, *args, **kwargs):
         """Convenience fluent method for :py:func:`broadcast_to`.
@@ -2539,13 +2543,13 @@ class Symbol(SymbolBase):
         """
         return op.softmin(self, *args, **kwargs)
 
-    def squeeze(self, *args, **kwargs):
+    def squeeze(self, axis=None, inplace=False, **kwargs): # pylint: disable=unused-argument
         """Convenience fluent method for :py:func:`squeeze`.
 
         The arguments are the same as for :py:func:`squeeze`, with
         this array as data.
         """
-        return op.squeeze(self, *args, **kwargs)
+        return op.squeeze(self, axis=axis, **kwargs)
 
     def get_backend_symbol(self, backend):
         """Return symbol for target backend.
diff --git a/scala-package/core/src/main/scala/org/apache/mxnet/NDArray.scala b/scala-package/core/src/main/scala/org/apache/mxnet/NDArray.scala
index d55ed961d..717120bcf 100644
--- a/scala-package/core/src/main/scala/org/apache/mxnet/NDArray.scala
+++ b/scala-package/core/src/main/scala/org/apache/mxnet/NDArray.scala
@@ -756,7 +756,7 @@ object NDArray extends NDArrayBase {
 class NDArray private[mxnet](private[mxnet] val handle: NDArrayHandle,
                              val writable: Boolean) extends NativeResource {
 
-  @deprecated("Please use ResourceScope instead", "1.6.0")
+  @deprecated("Please use ResourceScope instead", "1.5.0")
   def this(handle: NDArrayHandle,
            writable: Boolean = true,
            addToCollector: Boolean = true) {
diff --git a/scala-package/core/src/main/scala/org/apache/mxnet/NDArrayCollector.scala b/scala-package/core/src/main/scala/org/apache/mxnet/NDArrayCollector.scala
index 5fa0aa530..0761481cd 100644
--- a/scala-package/core/src/main/scala/org/apache/mxnet/NDArrayCollector.scala
+++ b/scala-package/core/src/main/scala/org/apache/mxnet/NDArrayCollector.scala
@@ -64,7 +64,7 @@ import scala.collection.mutable
  *  });
  *  </pre>
  */
-@deprecated("Please use ResourceScope instead", "1.6.0")
+@deprecated("Please use ResourceScope instead", "1.5.0")
 object NDArrayCollector {
   private val logger = LoggerFactory.getLogger(classOf[NDArrayCollector])
 
@@ -76,14 +76,14 @@ object NDArrayCollector {
    * Create a collector which will dispose the collected NDArrays automatically.
    * @return an auto-disposable collector.
    */
-  @deprecated("Please use ResourceScope instead", "1.6.0")
+  @deprecated("Please use ResourceScope instead", "1.5.0")
   def auto(): NDArrayCollector = new NDArrayCollector(true)
 
   /**
    * Create a collector allows users to later dispose the collected NDArray manually.
    * @return a manually-disposable collector.
    */
-  @deprecated("Please use ResourceScope instead", "1.6.0")
+  @deprecated("Please use ResourceScope instead", "1.5.0")
   @Experimental
   def manual(): NDArrayCollector = new NDArrayCollector(false)
 
@@ -91,13 +91,13 @@ object NDArrayCollector {
    * Collect the NDArrays into the collector of the current thread.
    * @param ndArray NDArrays need to be collected.
    */
-  @deprecated("Please use ResourceScope instead", "1.6.0")
+  @deprecated("Please use ResourceScope instead", "1.5.0")
   @varargs def collect(ndArray: NDArray*): Unit = {
     currCollector.get().add(ndArray: _*)
   }
 }
 
-@deprecated("Please use ResourceScope instead", "1.6.0")
+@deprecated("Please use ResourceScope instead", "1.5.0")
 class NDArrayCollector private(private val autoDispose: Boolean = true,
                                private val doCollect: Boolean = true) {
   // native ptr (handle) of the NDArray -> NDArray
@@ -147,7 +147,7 @@ class NDArrayCollector private(private val autoDispose: Boolean = true,
    * @return The result of function <em>codeBlock</em>.
    */
   @Experimental
-  @deprecated("Please use ResourceScope instead", "1.6.0")
+  @deprecated("Please use ResourceScope instead", "1.5.0")
   def withScope[T](codeBlock: => T): T = {
     val old = NDArrayCollector.currCollector.get()
     NDArrayCollector.currCollector.set(this)
diff --git a/scala-package/pom.xml b/scala-package/pom.xml
index 147d7615a..e1887d7c3 100644
--- a/scala-package/pom.xml
+++ b/scala-package/pom.xml
@@ -57,7 +57,7 @@
     <scala.version>2.11.8</scala.version>
     <build.platform/>
     <scala.binary.version>2.11</scala.binary.version>
-    <base.revision>1.5.0</base.revision>
+    <base.revision>1.6.0</base.revision>
     <build.platform />
     <cxx>g++</cxx>
     <dollar>$</dollar>
diff --git a/src/c_api/c_api_symbolic.cc b/src/c_api/c_api_symbolic.cc
index df839b4d2..627acf46c 100644
--- a/src/c_api/c_api_symbolic.cc
+++ b/src/c_api/c_api_symbolic.cc
@@ -1046,8 +1046,10 @@ int MXGenBackendSubgraph(SymbolHandle sym_handle, const char *backend_name,
   for (auto property : subgraph_prop_list) {
     nnvm::Graph g = Symbol2Graph(*s);
     property->SetAttr("graph", g);
-    g.attrs["subgraph_property"] = std::make_shared<nnvm::any>(std::move(property));
+    g.attrs["subgraph_property"] = std::make_shared<nnvm::any>(property);
     g = ApplyPass(std::move(g), "BuildSubgraph");
+    property->RemoveAttr("graph");
+    g.attrs.erase("subgraph_property");
     s->outputs = g.outputs;
   }
   *ret_sym_handle = s;
diff --git a/src/c_api/c_api_test.cc b/src/c_api/c_api_test.cc
index fff926929..a1f12fad6 100644
--- a/src/c_api/c_api_test.cc
+++ b/src/c_api/c_api_test.cc
@@ -50,8 +50,10 @@ int MXBuildSubgraphByOpNames(SymbolHandle sym_handle,
       g.outputs = s->outputs;
       property->SetAttr("graph", g);
       property->SetAttr("op_names", op_name_set);
-      g.attrs["subgraph_property"] = std::make_shared<nnvm::any>(std::move(property));
+      g.attrs["subgraph_property"] = std::make_shared<nnvm::any>(property);
       g = nnvm::ApplyPass(std::move(g), "BuildSubgraph");
+      property->RemoveAttr("graph");
+      g.attrs.erase("subgraph_property");
       s->outputs = g.outputs;
     }
   }
diff --git a/src/common/cuda_utils.h b/src/common/cuda_utils.h
index cf0e1a729..8238a41b4 100644
--- a/src/common/cuda_utils.h
+++ b/src/common/cuda_utils.h
@@ -526,7 +526,6 @@ static inline __device__  void atomicAdd(double *address, double val) {
 // Taken from:
 // https://github.com/torch/cutorch/blob/master/lib/THC/THCAtomics.cuh
 //#if defined(__CUDA_ARCH__)
-//#if (__HIP_DEVICE_COMPILE__)
 #if (__HIP_DEVICE_COMPILE__) || defined(__HCC__)
 static inline __device__ void atomicAdd(mshadow::half::half_t *address,
                                         mshadow::half::half_t val) {
diff --git a/src/executor/graph_executor.cc b/src/executor/graph_executor.cc
index 8ffc041cb..fc6603eed 100644
--- a/src/executor/graph_executor.cc
+++ b/src/executor/graph_executor.cc
@@ -1688,8 +1688,10 @@ static nnvm::Symbol BuildSubgraph(const nnvm::Symbol& src, op::SubgraphPropertyP
   g = InferForwardAttrs(g, arg_shapes, arg_dtypes, arg_stypes, default_ctx, ctx_map, in_arg_ctxes,
                         aux_state_ctxes, true);
   subgraph_prop->SetAttr("graph", g);
-  g.attrs["subgraph_property"] = std::make_shared<nnvm::any>(std::move(subgraph_prop));
+  g.attrs["subgraph_property"] = std::make_shared<nnvm::any>(subgraph_prop);
   g = ApplyPass(std::move(g), "BuildSubgraph");
+  subgraph_prop->RemoveAttr("graph");
+  g.attrs.erase("subgraph_property");
   ret.outputs = g.outputs;
   return ret;
 }
diff --git a/src/imperative/imperative.cc b/src/imperative/imperative.cc
index e2c0c9d4c..c00021c44 100644
--- a/src/imperative/imperative.cc
+++ b/src/imperative/imperative.cc
@@ -313,7 +313,9 @@ std::vector<NDArray*> Imperative::Backward(
     } else {
       info.outputs.emplace_back(outputs[i]->shape(), outputs[i]->ctx(),
                                 true, outputs[i]->dtype());
-      info.outputs.back() = static_cast<real_t>(1.0);
+      if (info.outputs.back().shape().Size() != 0) {
+        info.outputs.back() = static_cast<real_t>(1.0);
+      }
     }
   }
 
diff --git a/src/imperative/imperative_utils.h b/src/imperative/imperative_utils.h
index 601a0ce28..ac5354ddc 100644
--- a/src/imperative/imperative_utils.h
+++ b/src/imperative/imperative_utils.h
@@ -419,7 +419,14 @@ inline void PushFCompute(const FCompute& fn,
       // mapping from index in input_blobs to index in pre_temp_dst
       std::unordered_map<uint32_t, uint32_t> in_temp_idx_map;
 #if MXNET_USE_MKLDNN == 1
-      InvalidateOutputs(outputs, req);
+      if (exec_type != ExecType::kCrossDeviceCopy) {
+        // kCrossDeviceCopy is used for `_copy_to` operator, which doesn't compute immediately in
+        // its FCcomputeEx, but AsyncPush the copy operation to engine.
+        // So for the case that A is holding mkldnn memory, and then copy A to B, and then copy B
+        // back to A, we shouldn't invalidate outputs for copying B back to A, because at this time,
+        // copying A to B may not happen, and will corrupt A's memory.
+        InvalidateOutputs(outputs, req);
+      }
 #endif
       std::vector<OpReqType> tmp_req = req;
       // setup blobs
@@ -461,7 +468,14 @@ inline void PushFComputeEx(const FComputeEx& fn,
   const auto& run = [=](RunContext rctx) {
       OpContext opctx{need_grad, is_train, rctx, engine::CallbackOnComplete(), requested};
 #if MXNET_USE_MKLDNN == 1
-      InvalidateOutputs(outputs, req);
+      if (exec_type != ExecType::kCrossDeviceCopy) {
+        // kCrossDeviceCopy is used for `_copy_to` operator, which doesn't compute immediately in
+        // its FCcomputeEx, but AsyncPush the copy operation to engine.
+        // So for the case that A is holding mkldnn memory, and then copy A to B, and then copy B
+        // back to A, we shouldn't invalidate outputs for copying B back to A, because at this time,
+        // copying A to B may not happen, and will corrupt A's memory.
+        InvalidateOutputs(outputs, req);
+      }
 #endif
       fn(attrs, opctx, inputs, req, outputs);
       if (ctx.dev_mask() == gpu::kDevMask && exec_type == ExecType::kSync && !rctx.is_bulk) {
diff --git a/src/ndarray/ndarray.cc b/src/ndarray/ndarray.cc
index f8af6edc8..e2fccde23 100644
--- a/src/ndarray/ndarray.cc
+++ b/src/ndarray/ndarray.cc
@@ -96,6 +96,13 @@ NDArray::NDArray(const NDArrayStorageType stype, const mxnet::TShape &shape, Con
         dtype, aux_types, aux_shapes);
 }
 
+void NDArray::SetShapeFromChunk() {
+  if (Imperative::Get()->is_np_shape() ||
+      !(ptr_->storage_shape.ndim() == 1 && ptr_->storage_shape[0] == 0)) {
+    shape_ = ptr_->storage_shape;
+  }
+}
+
 struct ChunkMem {
   Storage::Handle h;
   std::vector<Storage::Handle> aux_h;
diff --git a/src/operator/contrib/boolean_mask.cc b/src/operator/contrib/boolean_mask.cc
index 4d66e1ec0..f431d77f2 100644
--- a/src/operator/contrib/boolean_mask.cc
+++ b/src/operator/contrib/boolean_mask.cc
@@ -143,6 +143,7 @@ inline void BooleanMaskForward<cpu>(const nnvm::NodeAttrs& attrs,
   // set the output shape forcefully
   mxnet::TShape s = data.shape();
   s[axis] = valid_num;
+
   const_cast<NDArray &>(out).Init(s);
   // do the copy
   MSHADOW_TYPE_SWITCH(data.dtype(), DType, {
diff --git a/src/operator/contrib/boolean_mask.cu b/src/operator/contrib/boolean_mask.cu
index d71633ba1..2a275a3ba 100644
--- a/src/operator/contrib/boolean_mask.cu
+++ b/src/operator/contrib/boolean_mask.cu
@@ -79,8 +79,7 @@ inline void BooleanMaskForward<gpu>(const nnvm::NodeAttrs& attrs,
                                 Stream<gpu>::GetStream(s));
   CUDA_CALL(hipMemcpy(&valid_num, &prefix_sum[idx_size - 1], sizeof(int32_t),
                        hipMemcpyDeviceToHost));
-  CHECK(valid_num > 0) << "boolean_mask behavior not defined when all masks are 0";
-  // Set the output shape forcefully
+   // Set the output shape forcefully
   mxnet::TShape data_shape = data.shape();
   data_shape[axis] = valid_num;
   const_cast<NDArray &>(out).Init(data_shape);
@@ -88,8 +87,10 @@ inline void BooleanMaskForward<gpu>(const nnvm::NodeAttrs& attrs,
   size_t col_size = input_size / idx.shape()[0];
   // Do the copy
   MSHADOW_TYPE_SWITCH(out.dtype(), DType, {
-    mxnet_op::Kernel<BooleanMaskForwardKernel, gpu>::Launch(
-      s, input_size, out.data().dptr<DType>(), data.data().dptr<DType>(), prefix_sum, col_size);
+    if (valid_num > 0) {
+      mxnet_op::Kernel<BooleanMaskForwardKernel, gpu>::Launch(
+        s, input_size, out.data().dptr<DType>(), data.data().dptr<DType>(), prefix_sum, col_size);
+    }
   });
 }
 
@@ -143,9 +144,11 @@ inline void BooleanMaskBackward<gpu>(const nnvm::NodeAttrs& attrs,
   size_t col_size = input_size / idx_size;
   // Backward pass
   MSHADOW_TYPE_SWITCH(igrad_data.dtype(), DType, {
-    mxnet_op::Kernel<BooleanMaskBackwardKernel, gpu>::Launch(
-      s, input_size, igrad_data.data().dptr<DType>(), req[0], ograd.data().dptr<DType>(),
-      prefix_sum, col_size);
+    if (input_size > 0) {
+      mxnet_op::Kernel<BooleanMaskBackwardKernel, gpu>::Launch(
+        s, input_size, igrad_data.data().dptr<DType>(), req[0], ograd.data().dptr<DType>(),
+        prefix_sum, col_size);
+    }
   });
 }
 
diff --git a/src/operator/image/image_random-inl.h b/src/operator/image/image_random-inl.h
index 6b3e4365b..c96a5ca22 100644
--- a/src/operator/image/image_random-inl.h
+++ b/src/operator/image/image_random-inl.h
@@ -339,7 +339,7 @@ void NormalizeOpForward(const nnvm::NodeAttrs &attrs,
   std::vector<float> mean(3);
   std::vector<float> std(3);
   if (param.mean.ndim() == 1) {
-    mean[0] = mean[1] = mean[3] = param.mean[0];
+    mean[0] = mean[1] = mean[2] = param.mean[0];
   } else {
     mean[0] = param.mean[0];
     mean[1] = param.mean[1];
diff --git a/src/operator/nn/cudnn/cudnn_batch_norm-inl.h b/src/operator/nn/cudnn/cudnn_batch_norm-inl.h
index 689e3736c..b012823b4 100644
--- a/src/operator/nn/cudnn/cudnn_batch_norm-inl.h
+++ b/src/operator/nn/cudnn/cudnn_batch_norm-inl.h
@@ -95,7 +95,6 @@ class CuDNNBatchNormOp {
 /*#if CUDNN_VERSION >= 7002
     auto mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;
 #else*/ //TODO Commented as not supported in MIOpen
-    //auto mode = CUDNN_BATCHNORM_SPATIAL;
     auto mode = miopenBNSpatial;
 //#endif
 
@@ -187,7 +186,6 @@ class CuDNNBatchNormOp {
 /*#if CUDNN_VERSION >= 7002
     auto mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;
 #else*/
-    //auto mode = CUDNN_BATCHNORM_SPATIAL;
     auto mode = miopenBNSpatial;
 //#endif
     MSHADOW_REAL_TYPE_SWITCH(dtype_param_, DTypeParam, {
diff --git a/src/operator/nn/mkldnn/mkldnn_base.cc b/src/operator/nn/mkldnn/mkldnn_base.cc
index e36a0f008..a13337b12 100644
--- a/src/operator/nn/mkldnn/mkldnn_base.cc
+++ b/src/operator/nn/mkldnn/mkldnn_base.cc
@@ -329,6 +329,7 @@ mkldnn_memory_format_t GetDefaultFormat(const mkldnn::memory::desc &desc) {
       case mkldnn_nchw:
       case mkldnn_nhwc:
       case mkldnn_chwn:
+      case mkldnn_nChw4c:
       case mkldnn_nChw8c:
       case mkldnn_nChw16c:
         return mkldnn_nchw;
@@ -338,6 +339,7 @@ mkldnn_memory_format_t GetDefaultFormat(const mkldnn::memory::desc &desc) {
       case mkldnn_iohw:
       case mkldnn_oIhw8i:
       case mkldnn_oIhw16i:
+      case mkldnn_OIhw4i4o:
       case mkldnn_OIhw8i8o:
       case mkldnn_hwio_s8s8:
       case mkldnn_OIhw16i16o:
@@ -376,6 +378,7 @@ mkldnn_memory_format_t GetDefaultFormat(const mkldnn::memory::desc &desc) {
       case mkldnn_giohw:
       case mkldnn_hwigo:
       case mkldnn_hwigo_s8s8:
+      case mkldnn_gOIhw4i4o:
       case mkldnn_gOIhw8i8o:
       case mkldnn_gOIhw16i16o:
       case mkldnn_gOIhw4i16o4i:
@@ -383,6 +386,7 @@ mkldnn_memory_format_t GetDefaultFormat(const mkldnn::memory::desc &desc) {
       case mkldnn_gOIhw8i16o2i:
       case mkldnn_gOIhw8o16i2o:
       case mkldnn_gOIhw8o8i:
+      case mkldnn_gOIhw4o4i:
       case mkldnn_gOIhw16o16i:
       case mkldnn_gIOhw16o16i:
       case mkldnn_gOihw8o:
diff --git a/src/operator/quantization/mkldnn/mkldnn_quantized_concat.cc b/src/operator/quantization/mkldnn/mkldnn_quantized_concat.cc
index d9e884e82..2a4c6d612 100644
--- a/src/operator/quantization/mkldnn/mkldnn_quantized_concat.cc
+++ b/src/operator/quantization/mkldnn/mkldnn_quantized_concat.cc
@@ -64,22 +64,32 @@ static void MKLDNNQuantizedConcatForward(const nnvm::NodeAttrs& attrs, const OpC
   std::vector<const mkldnn::memory*> data_mem;
   // new_data_mem is for auto-free new created mkldnn memory
   std::vector<std::shared_ptr<mkldnn::memory>> new_data_mem;
+  const auto out_dtype = out_data[quantized_concat_enum::kOut].dtype();
   for (int i = 0; i < param_.num_args; ++i) {
     auto i_scale = GetScale(in_data[i], data_min[i], data_max[i]);
     if (i_scale == out_scale) {
+      CHECK(in_data[i].dtype() == out_dtype);
       auto mem = in_data[i].GetMKLDNNData();
       data_mem.push_back(mem);
       data_md.push_back(mem->get_primitive_desc());
     } else {
       auto mem = in_data[i].GetMKLDNNData();
       auto pd = mem->get_primitive_desc();
+      if (in_data[i].dtype() != out_dtype) {
+        auto mem_desc = pd.desc();
+        mkldnn::memory::desc new_md(
+            mkldnn::memory::dims(mem_desc.data.dims, mem_desc.data.dims + mem_desc.data.ndims),
+            get_mkldnn_type(out_dtype), static_cast<mkldnn::memory::format>(mem_desc.data.format));
+        pd = mkldnn::memory::primitive_desc(new_md, CpuEngine::Get()->get_engine());
+      }
       const auto rescaled_mem = std::make_shared<mkldnn::memory>(pd);
       new_data_mem.push_back(rescaled_mem);
       std::vector<float> reorder_scale = {out_scale / i_scale};
       primitive_attr reorder_attr;
       reorder_attr.set_int_output_round_mode(round_mode::round_nearest);
       reorder_attr.set_output_scales(0, reorder_scale);
-      const auto reorder_pd = mkldnn::reorder::primitive_desc(pd, pd, reorder_attr);
+      const auto reorder_pd =
+          mkldnn::reorder::primitive_desc(mem->get_primitive_desc(), pd, reorder_attr);
       MKLDNNStream::Get()->RegisterPrim(mkldnn::reorder(reorder_pd, *mem, *rescaled_mem));
       data_mem.push_back(rescaled_mem.get());
       data_md.push_back(pd);
diff --git a/src/operator/subgraph/subgraph_property.h b/src/operator/subgraph/subgraph_property.h
index 0683e5ed6..e2c861ef5 100644
--- a/src/operator/subgraph/subgraph_property.h
+++ b/src/operator/subgraph/subgraph_property.h
@@ -330,6 +330,15 @@ class SubgraphProperty {
     auto it = attrs_.find(name);
     return it != attrs_.end();
   }
+  /*!
+   * \brief Remove attr if the attr exists.
+   */
+  void RemoveAttr(const std::string& name) {
+    auto it = attrs_.find(name);
+    if (it != attrs_.end()) {
+      attrs_.erase(it);
+    }
+  }
   /*!
    * \brief Get the property type.
    */
@@ -384,6 +393,16 @@ class SubgraphBackend {
     return it != attrs_.end();
   }
 
+  /*!
+   * \brief Remove attr if the attr exists.
+   */
+  void RemoveAttr(const std::string& name) {
+    auto it = attrs_.find(name);
+    if (it != attrs_.end()) {
+      attrs_.erase(it);
+    }
+  }
+
   SubgraphPropertyPtr& RegisterSubgraphProperty(const SubgraphPropertyPtr prop) {
     prop_ptr_.push_back(prop);
     return prop_ptr_.back();
diff --git a/src/operator/subgraph/tensorrt/nnvm_to_onnx-inl.h b/src/operator/subgraph/tensorrt/nnvm_to_onnx-inl.h
index edf4d357e..5a433f1d9 100644
--- a/src/operator/subgraph/tensorrt/nnvm_to_onnx-inl.h
+++ b/src/operator/subgraph/tensorrt/nnvm_to_onnx-inl.h
@@ -33,12 +33,16 @@
 
 #include <onnx/onnx_pb.h>
 
+#include <unordered_map>
+#include <vector>
 #include <string>
 
 namespace mxnet {
 namespace op {
 namespace nnvm_to_onnx {
 
+enum ConvDeconvType {Convolution, Deconvolution};
+
 using namespace nnvm;
 using namespace ::onnx;
 using int64 = ::google::protobuf::int64;
@@ -46,8 +50,7 @@ using int64 = ::google::protobuf::int64;
 std::unordered_map<std::string, mxnet::TShape> GetPlaceholderShapes(const ShapeVector& shape_inputs,
     const nnvm::IndexedGraph& ig);
 
-std::unordered_map<std::string, int> GetPlaceholderDTypes(const DTypeVector&
-dtype_inputs,
+std::unordered_map<std::string, int> GetPlaceholderDTypes(const DTypeVector& dtype_inputs,
     const nnvm::IndexedGraph& ig);
 
 std::unordered_map<std::string, uint32_t> GetOutputLookup(const nnvm::IndexedGraph& ig);
@@ -72,14 +75,24 @@ typedef void (*ConverterFunction)(NodeProto *node_proto,
                                   const nnvm::IndexedGraph &ig,
                                   const array_view<IndexedGraph::NodeEntry> &inputs);
 
+template <class ConvDeconvParam>
+void ConvDeconvConvertHelper(NodeProto *node_proto,
+                             const NodeAttrs &attrs,
+                             const nnvm::IndexedGraph &ig,
+                             const array_view<IndexedGraph::NodeEntry> &inputs,
+                             const ConvDeconvParam& param,
+                             ConvDeconvType type);
 
 // Forward declarations
-void ConvertConvolution(
-                        NodeProto *node_proto,
+void ConvertConvolution(NodeProto *node_proto,
                         const NodeAttrs &attrs,
                         const nnvm::IndexedGraph &ig,
                         const array_view<IndexedGraph::NodeEntry> &inputs);
 
+void ConvertDeconvolution(NodeProto *node_proto,
+                        const NodeAttrs &attrs,
+                        const nnvm::IndexedGraph &ig,
+                        const array_view<IndexedGraph::NodeEntry> &inputs);
 
 void ConvertPooling(NodeProto *node_proto,
                     const NodeAttrs &attrs,
@@ -152,13 +165,14 @@ void ConvertPad(NodeProto* node_proto,
                 const array_view<IndexedGraph::NodeEntry> &inputs);
 
 std::string ConvertNnvmGraphToOnnx(const nnvm::Graph &g,
-    const std::unordered_map<std::string, NDArray>* const params_map);
+    std::unordered_map<std::string, NDArray>* params_map);
 
 static const std::unordered_map<std::string, ConverterFunction> converter_map = {
   {"Activation", ConvertActivation},
   {"BatchNorm", ConvertBatchNorm},
   {"clip", ConvertClip},
   {"Convolution", ConvertConvolution},
+  {"Deconvolution", ConvertDeconvolution},
   {"Concat", ConvertConcatenate},
   {"Dropout", ConvertDropout},
   {"elemwise_add", ConvertElementwiseAdd},
@@ -172,6 +186,18 @@ static const std::unordered_map<std::string, ConverterFunction> converter_map =
   {"SoftmaxOutput", ConvertSoftmaxOutput}
 };
 
+typedef void (*PreprocessFunction)(const NodeAttrs &attrs,
+                                   const std::vector<nnvm::NodeEntry> &inputs,
+                                   std::unordered_map<std::string, NDArray> *params_map);
+
+void PreprocessBatchNorm(const NodeAttrs &attrs,
+                         const std::vector<nnvm::NodeEntry> &inputs,
+                         std::unordered_map<std::string, NDArray> *params_map);
+
+static const std::unordered_map<std::string, PreprocessFunction> preprocess_map = {
+  {"BatchNorm", PreprocessBatchNorm}
+};
+
 }  // namespace nnvm_to_onnx
 }  // namespace op
 }  // namespace mxnet
diff --git a/src/operator/subgraph/tensorrt/nnvm_to_onnx.cc b/src/operator/subgraph/tensorrt/nnvm_to_onnx.cc
index 9d98a48c2..84580d0b0 100644
--- a/src/operator/subgraph/tensorrt/nnvm_to_onnx.cc
+++ b/src/operator/subgraph/tensorrt/nnvm_to_onnx.cc
@@ -31,6 +31,7 @@
 #include <mxnet/base.h>
 #include <nnvm/graph.h>
 #include <nnvm/pass_functions.h>
+#include <operator/nn/deconvolution-inl.h>
 
 #include "../../../common/utils.h"
 #include "../../../ndarray/ndarray_function.h"
@@ -54,7 +55,7 @@ namespace nnvm_to_onnx {
 
 std::string ConvertNnvmGraphToOnnx(
     const nnvm::Graph& g,
-    const std::unordered_map<std::string, NDArray>* const params_map) {
+    std::unordered_map<std::string, NDArray>* params_map) {
 
   static std::atomic_ulong subgraph_count = { 0 };
 
@@ -88,8 +89,21 @@ std::string ConvertNnvmGraphToOnnx(
   auto placeholder_shapes = GetPlaceholderShapes(shape_inputs, ig);
   auto placeholder_dtypes = GetPlaceholderDTypes(dtype_inputs, ig);
   auto output_lookup = GetOutputLookup(ig);
-  uint32_t current_input = 0;
 
+  for (uint32_t node_idx = 0; node_idx < ig.num_nodes(); ++node_idx) {
+      const IndexedGraph::Node& node = ig[node_idx];
+      const nnvm::Node* source = node.source;
+      // If this is a op
+      if (!source->is_variable()) {
+        auto mightNeedPreprocessNode = preprocess_map.find(source->op()->name);
+        // if this op is defined in preprocess_map
+        if (mightNeedPreprocessNode != preprocess_map.end()) {
+          mightNeedPreprocessNode->second(source->attrs, source->inputs, params_map);
+        }
+      }
+  }
+
+  uint32_t current_input = 0;
   // Can't do a foreach over IndexedGraph since it doesn't implement begin(), etc.
   for (uint32_t node_idx = 0; node_idx < ig.num_nodes(); ++node_idx) {
     const IndexedGraph::Node& node = ig[node_idx];
@@ -157,20 +171,25 @@ std::string ConvertNnvmGraphToOnnx(
   return serialized_onnx_graph;
 }
 
-void ConvertConvolution(NodeProto* node_proto, const NodeAttrs& attrs,
-                        const nnvm::IndexedGraph& /*ig*/,
-                        const array_view<IndexedGraph::NodeEntry>& /*inputs*/) {
-  const auto& conv_param = nnvm::get<op::ConvolutionParam>(attrs.parsed);
-
-  node_proto->set_op_type("Conv");
+template <class ConvDeconvParam>
+void ConvDeconvConvertHelper(NodeProto* node_proto, const NodeAttrs& attrs,
+                             const nnvm::IndexedGraph& /*ig*/,
+                             const array_view<IndexedGraph::NodeEntry>& /*input*/,
+                             const ConvDeconvParam& param,
+                             ConvDeconvType type) {
+  if (type == ConvDeconvType::Convolution) {
+    node_proto->set_op_type("Conv");
+  } else {
+    node_proto->set_op_type("ConvTranspose");
+  }
 
-  const mxnet::TShape kernel = conv_param.kernel;
-  const mxnet::TShape stride = conv_param.stride;
-  const mxnet::TShape dilate = conv_param.dilate;
-  const mxnet::TShape pad = conv_param.pad;
-  const uint32_t num_group = conv_param.num_group;
+  const mxnet::TShape kernel = param.kernel;
+  const mxnet::TShape stride = param.stride;
+  const mxnet::TShape dilate = param.dilate;
+  const mxnet::TShape pad = param.pad;
+  const uint32_t num_group = param.num_group;
   // const bool no_bias = conv_param.no_bias;
-  const dmlc::optional<int> layout = conv_param.layout;
+  const dmlc::optional<int> layout = param.layout;
 
   // dilations
   AttributeProto* const dilations = node_proto->add_attribute();
@@ -213,8 +232,24 @@ void ConvertConvolution(NodeProto* node_proto, const NodeAttrs& attrs,
   for (const dim_t kval : stride) {
     strides->add_ints(static_cast<int64>(kval));
   }
+}
+
+void ConvertConvolution(NodeProto* node_proto, const NodeAttrs& attrs,
+                        const nnvm::IndexedGraph& ig,
+                        const array_view<IndexedGraph::NodeEntry>& inputs) {
+  const auto& conv_param = nnvm::get<op::ConvolutionParam>(attrs.parsed);
+  ConvDeconvConvertHelper(node_proto, attrs, ig, inputs, conv_param,
+      ConvDeconvType::Convolution);
 }  // end ConvertConvolution
 
+void ConvertDeconvolution(NodeProto* node_proto, const NodeAttrs& attrs,
+                          const nnvm::IndexedGraph& ig,
+                          const array_view<IndexedGraph::NodeEntry>& inputs) {
+  const auto& deconv_param = nnvm::get<op::DeconvolutionParam>(attrs.parsed);
+  ConvDeconvConvertHelper(node_proto, attrs, ig, inputs, deconv_param,
+      ConvDeconvType::Deconvolution);
+}  // end ConvertDeconvolution
+
 void ConvertPooling(NodeProto* node_proto, const NodeAttrs& attrs,
                     const nnvm::IndexedGraph& /*ig*/,
                     const array_view<IndexedGraph::NodeEntry>& /*inputs*/) {
@@ -642,6 +677,18 @@ void ConvertDropout(NodeProto* node_proto, const NodeAttrs& attrs,
   node_proto->set_op_type("Dropout");
 }
 
+void PreprocessBatchNorm(const NodeAttrs &attrs,
+                         const std::vector<nnvm::NodeEntry> &inputs,
+                         std::unordered_map<std::string, NDArray> *params_map) {
+  const auto& param = nnvm::get<op::BatchNormParam>(attrs.parsed);
+  if (param.fix_gamma) {
+    // if mxnet is specify fix_gamma, we will need to preprocess the params map
+    // to convert the gamma associate with this batch norm layer to 1.
+    std::string gammaNodeName = inputs[batchnorm::kGamma].node->attrs.name;
+    (*params_map)[gammaNodeName] = 1.0f;
+  }
+}
+
 }  // namespace nnvm_to_onnx
 }  // namespace op
 }  // namespace mxnet
diff --git a/src/operator/subgraph/tensorrt/tensorrt-inl.h b/src/operator/subgraph/tensorrt/tensorrt-inl.h
index e258d892a..a6b93f105 100644
--- a/src/operator/subgraph/tensorrt/tensorrt-inl.h
+++ b/src/operator/subgraph/tensorrt/tensorrt-inl.h
@@ -88,6 +88,7 @@ class TensorrtSelector : public SubgraphSelector {
     "clip",
     "Concat",
     "Convolution",
+    "Deconvolution",
     "Dropout",
     "elemwise_add",
     "elemwise_sub",
@@ -104,6 +105,7 @@ class TensorrtSelector : public SubgraphSelector {
   const std::unordered_set<std::string> withWeightsOps = {
     "BatchNorm",
     "Convolution",
+    "Deconvolution",
     "FullyConnected"
   };
 
diff --git a/src/operator/subgraph/tensorrt/tensorrt.cc b/src/operator/subgraph/tensorrt/tensorrt.cc
index eac4ba7fc..8b64c2a6b 100644
--- a/src/operator/subgraph/tensorrt/tensorrt.cc
+++ b/src/operator/subgraph/tensorrt/tensorrt.cc
@@ -272,7 +272,7 @@ OpStatePtr TRTCreateState(const nnvm::NodeAttrs& attrs, Context ctx,
               << " instead of: " << max_batch_size;
     max_batch_size = in_shape[0][0];
   }
-  const auto& params_map = node_param.params_map;
+  std::unordered_map<std::string, NDArray> params_map = node_param.params_map;
   const auto& inputs_to_idx = node_param.inputs_to_idx;
   const auto& outputs_to_idx = node_param.outputs_to_idx;
   const auto& idx_g = graph.indexed_graph();
diff --git a/src/operator/tensor/elemwise_unary_op_trig.cc b/src/operator/tensor/elemwise_unary_op_trig.cc
index 13410e942..ff210040d 100644
--- a/src/operator/tensor/elemwise_unary_op_trig.cc
+++ b/src/operator/tensor/elemwise_unary_op_trig.cc
@@ -139,7 +139,33 @@ The storage type of ``tan`` output depends upon the input storage type:
 )code" ADD_FILELINE)
 .set_attr<nnvm::FGradient>("FGradient", ElemwiseGradUseOut{ "_backward_tan" });
 
-MXNET_OPERATOR_REGISTER_BINARY_WITH_SPARSE_CPU_DR(_backward_tan, unary_bwd<mshadow_op::tan_grad>);
+MXNET_OPERATOR_REGISTER_BINARY_WITH_SPARSE_CPU_DR(_backward_tan, unary_bwd<mshadow_op::tan_grad>)
+.set_attr<nnvm::FGradient>("FGradient",
+  [](const nnvm::NodePtr& n, const std::vector<nnvm::NodeEntry>& ograds) {
+      // NodeEntry{n} : y_grad * f'(x)
+      // n->inputs[0] : y_grad (dL/dy)
+      // n->inputs[1] : y = f(x) = tan(x) (ElemwiseGradUseOut)
+      // ograds[0] : head_grads (dL/dxgrad)
+      // f'(x) = sec^2(x)
+      // f''(x) = 2 * f'(x) * f(x)
+      //
+      // Note: When building gradient graph, the backward node of n->inputs[1] will be
+      // added to the graph again, therefore f`(x) will be multiplied
+      // So we need to compute only -> 2 * f(x) * dL/dy_grad * y_grad
+      const std::unordered_map<std::string, std::string> args = {{"scalar", "2.0"}};
+      auto two_y = MakeNode("_mul_scalar", n->attrs.name + "_mul_two", {n->inputs[1]}, &args, &n);
+      auto grad_grad_mid = MakeNode("elemwise_mul", n->attrs.name + "_grad_mul",
+                                    {n->inputs[0], nnvm::NodeEntry{two_y}}, nullptr, &n);
+      auto dydx = MakeNode("elemwise_div", n->attrs.name + "_grad_div",
+                           {nnvm::NodeEntry{n}, n->inputs[0]}, nullptr, &n);
+
+      std::vector<nnvm::NodeEntry> ret;
+      ret.emplace_back(MakeNode("elemwise_mul", n->attrs.name + "backward_grad_grad",
+                                {ograds[0], nnvm::NodeEntry{dydx}}, nullptr, &n));
+      ret.emplace_back(MakeNode("elemwise_mul", n->attrs.name + "backward_grad_grad_in",
+                                {ograds[0], nnvm::NodeEntry{grad_grad_mid}}, nullptr, &n));
+      return ret;
+  });
 
 // arcsin
 MXNET_OPERATOR_REGISTER_UNARY_WITH_RSP_CSR(arcsin, cpu, mshadow_op::arcsin)
@@ -290,7 +316,34 @@ The storage type of ``tanh`` output depends upon the input storage type:
 )code" ADD_FILELINE)
 .set_attr<nnvm::FGradient>("FGradient", ElemwiseGradUseOut{ "_backward_tanh" });
 
-MXNET_OPERATOR_REGISTER_BINARY_WITH_SPARSE_CPU_DR(_backward_tanh, unary_bwd<mshadow_op::tanh_grad>);
+MXNET_OPERATOR_REGISTER_BINARY_WITH_SPARSE_CPU_DR(_backward_tanh, unary_bwd<mshadow_op::tanh_grad>)
+.set_attr<nnvm::FGradient>("FGradient",
+  [](const nnvm::NodePtr& n, const std::vector<nnvm::NodeEntry>& ograds) {
+      // NodeEntry{n} : y_grad * f'(x)
+      // n->inputs[0] : y_grad (dL/dy)
+      // n->inputs[1] : y = f(x) = tanh(x) (ElemwiseGradUseOut)
+      // ograds[0] : head_grads dL/dxgrad
+      // f'(x) = sech^2(x)
+      // f''(x) = -2 * f'(x) * f(x)
+      //
+      // Note: when building gradient graph, the backward node of n->inputs[1] will be
+      // added to the graph again, therefore f`(x) will be multiplied
+      // So we need to compute only -> -2 * f(x) * dL/dy_grad * y_grad
+      const std::unordered_map<std::string, std::string> args = {{"scalar", "-2.0"}};
+      auto neg_two_y = MakeNode("_mul_scalar", n->attrs.name + "_mul_neg_two",
+                                {n->inputs[1]}, &args, &n);
+      auto grad_grad_mid = MakeNode("elemwise_mul", n->attrs.name + "_grad_mul",
+                                    {n->inputs[0], nnvm::NodeEntry{neg_two_y}}, nullptr, &n);
+      auto dydx = MakeNode("elemwise_div", n->attrs.name + "_grad_div",
+                           {nnvm::NodeEntry{n}, n->inputs[0]}, nullptr, &n);
+
+      std::vector<nnvm::NodeEntry> ret;
+      ret.emplace_back(MakeNode("elemwise_mul", n->attrs.name + "backward_grad_grad",
+                                {ograds[0], nnvm::NodeEntry{dydx}}, nullptr, &n));
+      ret.emplace_back(MakeNode("elemwise_mul", n->attrs.name + "backward_grad_grad_in",
+                                {ograds[0], nnvm::NodeEntry{grad_grad_mid}}, nullptr, &n));
+      return ret;
+  });
 
 // arcsinh
 MXNET_OPERATOR_REGISTER_UNARY_WITH_RSP_CSR(arcsinh, cpu, mshadow_op::arcsinh)
diff --git a/src/operator/tensor/matrix_op-inl.h b/src/operator/tensor/matrix_op-inl.h
index 3581739ba..ff624c25d 100644
--- a/src/operator/tensor/matrix_op-inl.h
+++ b/src/operator/tensor/matrix_op-inl.h
@@ -345,19 +345,34 @@ inline bool TransposeShape(const nnvm::NodeAttrs& attrs,
   CHECK_EQ(in_attrs->size(), 1U);
   CHECK_EQ(out_attrs->size(), 1U);
   mxnet::TShape& shp = (*in_attrs)[0];
+  mxnet::TShape& out_shp = (*out_attrs)[0];
   CHECK_LE(shp.ndim(), 6) << "Transpose support at most 6 dimensions";
-  mxnet::TShape ret(shp.ndim(), -1);
+  CHECK_NE(shp.ndim(), 0) << "Number of dimensions cannot be 0";
+  CHECK_NE(out_shp.ndim(), 0) << "Number of dimensions cannot be 0";
+  if (shp.ndim() == -1 && out_shp.ndim() == -1)
+    return false;  // none of the shapes is known
+  if (out_shp.ndim() > 0 && shp.ndim() > 0)
+    CHECK_EQ(out_shp.ndim(), shp.ndim());
+  mxnet::TShape get(std::max(shp.ndim(), out_shp.ndim()), -1);
+  mxnet::TShape ret(std::max(shp.ndim(), out_shp.ndim()), -1);
   if (param.axes.ndim() == 0) {
     for (int i = 0; i < shp.ndim(); ++i) {
       ret[i] = shp[shp.ndim()-1-i];
     }
+    for (int i = 0; i < out_shp.ndim(); ++i) {
+      get[shp.ndim()-1-i] = out_shp[i];
+    }
   } else {
-    CHECK_EQ(shp.ndim(), param.axes.ndim());
+    CHECK_EQ(std::max(shp.ndim(), out_shp.ndim()), param.axes.ndim());
     for (int i = 0; i < shp.ndim(); ++i) {
       CHECK(param.axes[i] < static_cast<int64_t>(shp.ndim()));
       ret[i] = shp[param.axes[i]];
     }
+    for (int i = 0; i < out_shp.ndim(); ++i) {
+      get[param.axes[i]] = out_shp[i];
+    }
   }
+  SHAPE_ASSIGN_CHECK(*in_attrs, 0, get);
   SHAPE_ASSIGN_CHECK(*out_attrs, 0, ret);
   return shape_is_known(ret);
 }
diff --git a/tests/python/mkl/test_subgraph.py b/tests/python/mkl/test_subgraph.py
index b25fefc6c..563fff1a6 100644
--- a/tests/python/mkl/test_subgraph.py
+++ b/tests/python/mkl/test_subgraph.py
@@ -401,6 +401,15 @@ def single_concat(data_shape, input_num, dim):
   concat = mx.symbol.Concat(*inputs, name="concat", dim=dim)
   return concat
 
+def single_concat_pos_neg(data_shape):
+  data, weight = head_symbol(data_shape)
+  conv = mx.symbol.Convolution(data=data, weight=weight, name='conv', num_filter=4,
+                               kernel=(1, 1), stride=(1, 1), no_bias=True)
+  relu = mx.symbol.Activation(data=conv, name='relu', act_type='relu')
+  inputs = [data, relu]
+  concat = mx.symbol.Concat(*inputs, name="concat", dim=1)
+  return concat
+
 # concat scale alignment case
 def concat_scale_align(data_shape):
   data, weight = head_symbol(data_shape)
@@ -738,6 +747,8 @@ def test_pos_single_concat():
       net = single_concat(data_shape, 4, 3)
       check_quantize(net, data_shape, out_type, name='conv', check_calibration=False)
       check_quantize(net, data_shape, out_type, name='conv', check_calibration=False, gluon_forward=True)
+      net = single_concat_pos_neg(data_shape)
+      check_quantize(net, data_shape, out_type, name='', check_calibration=False)
 
 @with_seed()
 def test_pos_concat_scale_align():
diff --git a/tests/python/unittest/test_gluon.py b/tests/python/unittest/test_gluon.py
index b59ce2d08..af30980b1 100644
--- a/tests/python/unittest/test_gluon.py
+++ b/tests/python/unittest/test_gluon.py
@@ -115,7 +115,7 @@ def test_parameter_dict():
     params1.get('w1', shape=(10, 10), stype='row_sparse')
     params1.load('test_parameter_dict.params', ctx)
     trainer1 = mx.gluon.Trainer(params1, 'sgd')
-    
+
     # compare the values before and after save/load
     cur_w0 = params1.get('w0').data(ctx)
     cur_w1 = params1.get('w1').row_sparse_data(all_row_ids)
@@ -134,7 +134,7 @@ def test_parameter_dict():
     cur_w1 = params2.get('w1').data(ctx)
     mx.test_utils.assert_almost_equal(prev_w0.asnumpy(), cur_w0.asnumpy())
     mx.test_utils.assert_almost_equal(prev_w1.asnumpy(), cur_w1.asnumpy())
-    
+
     # test the dtype casting functionality
     params0 = gluon.ParameterDict('')
     params0.get('w0', shape=(10, 10), dtype='float32')
@@ -386,7 +386,7 @@ def test_symbol_block():
         if 'conv' in param_name and 'weight' in param_name:
             break
     assert np.dtype(net_fp64.params[param_name].dtype) == np.dtype(np.float64)
-    
+
     # 3.b Verify same functionnality with the imports API
     net_fp_64 = mx.gluon.SymbolBlock.imports(sym_file, 'data', params_file, ctx=ctx)
 
@@ -2788,7 +2788,7 @@ def test_gluon_param_load():
     net.cast('float16')
     net.load_parameters('test_gluon_param_load.params', cast_dtype=True)
     mx.nd.waitall()
-    
+
 @with_seed()
 def test_gluon_param_load_dtype_source():
     net = mx.gluon.nn.Dense(10, in_units=10)
@@ -2800,6 +2800,22 @@ def test_gluon_param_load_dtype_source():
     assert net.weight.dtype == np.float16
     mx.nd.waitall()
 
+@with_seed()
+def test_squeeze_consistency():
+    class Foo(gluon.HybridBlock):
+        def __init__(self, inplace, **kwargs):
+            super(Foo, self).__init__(**kwargs)
+            self.inplace = inplace
+
+        def forward(self, x):
+            return x.squeeze(inplace=self.inplace)
+
+    for inplace in (True, False):
+        block = Foo(inplace)
+        block.hybridize()
+        shape = (np.random.randint(1, 10), np.random.randint(1, 10), 1)
+        block(mx.nd.ones(shape))
+
 if __name__ == '__main__':
     import nose
     nose.runmodule()
diff --git a/tests/python/unittest/test_gluon_estimator.py b/tests/python/unittest/test_gluon_estimator.py
index d2e8c082a..ae47d9256 100644
--- a/tests/python/unittest/test_gluon_estimator.py
+++ b/tests/python/unittest/test_gluon_estimator.py
@@ -19,11 +19,13 @@
 
 import sys
 import unittest
+import warnings
 
 import mxnet as mx
 from mxnet import gluon
 from mxnet.gluon import nn
 from mxnet.gluon.contrib.estimator import *
+from mxnet.gluon.contrib.estimator.event_handler import *
 from nose.tools import assert_raises
 
 
@@ -335,10 +337,9 @@ def test_default_handlers():
                     metrics=train_acc,
                     trainer=trainer,
                     context=ctx)
-    # no handler
+    # no handler(all default handlers), no warning
     with warnings.catch_warnings(record=True) as w:
         est.fit(train_data=train_data, epochs=num_epochs)
-        assert 'You are training with the' in str(w[-1].message)
 
     # handler with prepared loss and metrics
     # use mix of default and user defined handlers
@@ -353,7 +354,7 @@ def test_default_handlers():
     # handler with all user defined metrics
     # use mix of default and user defined handlers
     metric = MetricHandler(train_metrics=[train_acc])
-    logging = LoggingHandler(train_metrics=[train_acc], val_metrics=[mx.metric.RMSE("val acc")])
+    logging = LoggingHandler(train_metrics=[train_acc])
     est.fit(train_data=train_data, epochs=num_epochs, event_handlers=[metric, logging])
 
     # handler with mixed metrics, some handler use metrics prepared by estimator
diff --git a/tests/python/unittest/test_higher_order_grad.py b/tests/python/unittest/test_higher_order_grad.py
index 0f07d014d..429070de5 100644
--- a/tests/python/unittest/test_higher_order_grad.py
+++ b/tests/python/unittest/test_higher_order_grad.py
@@ -50,6 +50,41 @@ def test_cos():
         check_second_order_unary(array, cos, grad_grad_op)
 
 
+@with_seed()
+def test_tan():
+    def tan(x):
+        return nd.tan(x)
+
+    def grad_op(x):
+        return 1 / nd.cos(x)**2
+
+    def grad_grad_op(x):
+        return 2 * tan(x) * grad_op(x)
+
+    for dim in range(1, 5):
+        shape = rand_shape_nd(dim)
+        array = random_arrays(shape)
+        check_second_order_unary(array, tan, grad_grad_op)
+
+
+@with_seed()
+def test_tanh():
+    def tanh(x):
+        return nd.tanh(x)
+
+    def grad_op(x):
+        return 1 / nd.cosh(x)**2
+
+    def grad_grad_op(x):
+        return -2 * tanh(x) * grad_op(x)
+
+    for dim in range(1, 5):
+        shape = rand_shape_nd(dim)
+        array = random_arrays(shape)
+        check_second_order_unary(
+            array, tanh, grad_grad_op, rtol=1e-6, atol=1e-6)
+
+
 @with_seed()
 def test_relu():
     def relu(x):
@@ -150,7 +185,7 @@ def test_sigmoid():
         check_second_order_unary(array, sigmoid, grad_grad_op)
 
 
-def check_second_order_unary(x, op, grad_grad_op):
+def check_second_order_unary(x, op, grad_grad_op, rtol=None, atol=None):
     x = nd.array(x)
     grad_grad_x = grad_grad_op(x)
     x.attach_grad()
@@ -171,7 +206,8 @@ def check_second_order_unary(x, op, grad_grad_op):
         y_grad.asnumpy()
 
     # Validate the gradients.
-    assert_almost_equal(expected_grad_grad, x.grad.asnumpy())
+    assert_almost_equal(expected_grad_grad,
+                        x.grad.asnumpy(), rtol=rtol, atol=atol)
 
 
 if __name__ == '__main__':
diff --git a/tests/python/unittest/test_ndarray.py b/tests/python/unittest/test_ndarray.py
index f40bb3053..0f154bd67 100644
--- a/tests/python/unittest/test_ndarray.py
+++ b/tests/python/unittest/test_ndarray.py
@@ -172,6 +172,15 @@ def test_ndarray_negate():
     assert_almost_equal(npy, arr.asnumpy())
 
 
+@with_seed()
+def test_ndarray_magic_abs():
+    for dim in range(1, 7):
+        shape = rand_shape_nd(dim)
+        npy = np.random.uniform(-10, 10, shape)
+        arr = mx.nd.array(npy)
+        assert_almost_equal(abs(arr).asnumpy(), arr.abs().asnumpy())
+
+
 @with_seed()
 def test_ndarray_reshape():
     tensor = (mx.nd.arange(30) + 1).reshape(2, 3, 5)
@@ -193,6 +202,65 @@ def test_ndarray_reshape():
     assert same(tensor.reshape(-1, 0, reverse=True).asnumpy(), true_res.reshape(6, 5).asnumpy())
 
 
+@with_seed()
+def test_ndarray_flatten():
+    tensor = (mx.nd.arange(30) + 1).reshape(2, 3, 5)
+    copy = tensor.flatten()
+    ref = tensor.flatten(inplace=True)
+    assert same(copy.asnumpy(), tensor.reshape(2, 15).asnumpy())
+    assert same(ref.asnumpy(), tensor.reshape(2, 15).asnumpy())
+
+    tensor[0] = -1
+    assert not same(copy.asnumpy(), tensor.reshape(2, 15).asnumpy())
+    assert same(ref.asnumpy(), tensor.reshape(2, 15).asnumpy())
+
+
+@with_seed()
+def test_ndarray_squeeze():
+    def check_squeeze(shape, axis=None):
+        data = mx.random.uniform(low=-10.0, high=10.0, shape=shape)
+        copy = data.squeeze(axis=axis)
+        ref = data.squeeze(axis=axis, inplace=True)
+        out_expected = np.squeeze(data.asnumpy(), axis=axis)
+        if copy.shape == (1,):  # as an exception (1, 1, 1) will be squeezed to (1,)
+            out_expected = np.squeeze(data.asnumpy(), axis=tuple([i for i in range(1, len(shape))]))
+        assert same(copy.asnumpy(), out_expected)
+        assert same(ref.asnumpy(), out_expected)
+        data[0][0] = -1
+        assert same(copy.asnumpy(), out_expected)
+        assert not same(ref.asnumpy(), out_expected)
+
+    # check forward
+    check_squeeze((1, 5, 1, 3, 1), 0)
+    check_squeeze((1, 5, 1, 3, 1), 2)
+    check_squeeze((1, 5, 1, 3, 1), 4)
+    check_squeeze((1, 5, 1, 3, 1), (0, 4))
+    check_squeeze((1, 5, 1, 3, 1), (0, 2, 4))
+    check_squeeze((1, 5, 1, 3, 1), -5)
+    check_squeeze((1, 5, 1, 3, 1), -3)
+    check_squeeze((1, 5, 1, 3, 1), -1)
+    check_squeeze((1, 5, 1, 3, 1), (0, 4))
+    check_squeeze((1, 5, 1, 3, 1), (0, 2, 4))
+    check_squeeze((1, 5, 1, 3, 1))
+    check_squeeze((1, 1, 1, 1))
+
+
+@with_seed()
+def test_ndarray_expand_dims():
+    for ndim in range(1, 6):
+        for axis in range(-ndim-1, ndim+1):
+            shape = list(np.random.randint(1, 10, size=ndim))
+            data = mx.random.normal(shape=shape)
+            copy = data.expand_dims(axis=axis)
+            ref = data.expand_dims(axis=axis, inplace=True)
+            out_expected = np.expand_dims(data.asnumpy(), axis=axis)
+            assert same(copy.asnumpy(), out_expected)
+            assert same(ref.asnumpy(), out_expected), (shape, axis, ref.asnumpy().shape, out_expected.shape)
+            data[0] = -1
+            assert same(copy.asnumpy(), out_expected)
+            assert not same(ref.asnumpy(), out_expected)
+
+
 @with_seed()
 def test_ndarray_choose():
     shape = (100, 20)
diff --git a/tests/python/unittest/test_operator.py b/tests/python/unittest/test_operator.py
index 49a347563..72bf5864f 100644
--- a/tests/python/unittest/test_operator.py
+++ b/tests/python/unittest/test_operator.py
@@ -2001,10 +2001,12 @@ def test_depthwise_convolution():
 
 @with_seed()
 def test_convolution_independent_gradients():
-    ctx = default_context()
-    # set a low bar for autotuned cudnn conv
-    atol = 1.0e-1 if ctx.device_type == "gpu" else 1.0e-3
-    rtol = 1.0e-2 if ctx.device_type == "gpu" else 1.0e-3
+    # NOTE(zixuanweeei): Flaky test tracked by https://github.com/apache/incubator-mxnet/issues/15603.
+    # GPU context will be enabled after figuring out the possible issue tracked at
+    # https://github.com/apache/incubator-mxnet/issues/15638.
+    ctx = mx.cpu()
+    atol = 1.0e-3
+    rtol = 1.0e-3
     reqs = ["null", "write", "add"]
     var_names = ["x", "w", "b"]
     dims = [1, 2]
@@ -2034,14 +2036,14 @@ def test_convolution_independent_gradients():
         for req_kind in reqs:
             # Binding args for conv with possible dependent gradients
             base_args = {
-                'x': mx.nd.random.normal(shape=x_shape),
-                'w': mx.nd.random.normal(shape=w_shape),
-                'b': mx.nd.random.normal(shape=(num_filter, )) if not no_bias else None}
+                'x': mx.nd.random.normal(shape=x_shape, ctx=ctx),
+                'w': mx.nd.random.normal(shape=w_shape, ctx=ctx),
+                'b': mx.nd.random.normal(shape=(num_filter, ), ctx=ctx) if not no_bias else None}
             args1 = copy.deepcopy(base_args)
             grad1 = {
-                'x': mx.nd.zeros(shape=x_shape),
-                'w': mx.nd.zeros(shape=w_shape),
-                'b': mx.nd.zeros(shape=(num_filter, )) if not no_bias else None}
+                'x': mx.nd.zeros(shape=x_shape, ctx=ctx),
+                'w': mx.nd.zeros(shape=w_shape, ctx=ctx),
+                'b': mx.nd.zeros(shape=(num_filter, ), ctx=ctx) if not no_bias else None}
 
             grad_req1 = [req_kind] * 3
             grad_req1 = dict(zip(var_names, grad_req1))
@@ -2054,9 +2056,9 @@ def test_convolution_independent_gradients():
                 # Binding args for conv with independent gradients
                 args2 = copy.deepcopy(base_args)    # Deepcopy the same params of `exe1`
                 grad2 = {
-                    'x': mx.nd.zeros(shape=x_shape),
-                    'w': mx.nd.zeros(shape=w_shape),
-                    'b': mx.nd.zeros(shape=(num_filter, )) if not no_bias else None}
+                    'x': mx.nd.zeros(shape=x_shape, ctx=ctx),
+                    'w': mx.nd.zeros(shape=w_shape, ctx=ctx),
+                    'b': mx.nd.zeros(shape=(num_filter, ), ctx=ctx) if not no_bias else None}
                 grad_req2 = {"x": x_req, "w": w_req, "b": b_req}
                 exe2 = conv.bind(ctx, args2, args_grad=grad2, grad_req=grad_req2)
 
@@ -5587,6 +5589,21 @@ def test_boolean_mask():
     assert same(out.asnumpy(), expected)
     assert same(data.grad.asnumpy(), expected_grad)
 
+    # test 0-size output
+    mx.set_np_shape(True)
+    data = mx.nd.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]])
+    index = mx.nd.array([0, 0, 0])
+    data.attach_grad()
+    with mx.autograd.record():
+        out = mx.nd.contrib.boolean_mask(data, index)
+    out.backward()
+    data.grad.wait_to_read()
+    expected = np.zeros((0, 3))
+    expected_grad = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])
+    assert same(out.asnumpy(), expected)
+    assert same(data.grad.asnumpy(), expected_grad)
+    mx.set_np_shape(False)
+
     # test gradient
     shape = (100, 30)
     a = mx.nd.random.randint(0, 100, shape=shape)
@@ -8968,6 +8985,26 @@ def test_get_operator_arguments():
     ok_(operator_arguments.narg == 2)
 
 
+def test_transpose_infer_shape_back():
+    o1 = mx.sym.ones(shape=[2,3])
+    o2 = mx.sym.ones(shape=[-1,-1])
+    t = mx.sym.transpose(o2)
+    b = o1 + t
+    x = b.bind(mx.cpu(), args={})
+    y = x.forward()
+    assert(y[0].shape == (2,3))
+
+
+def test_transpose_infer_shape_mixed():
+    o1 = mx.sym.ones(shape=[2,-1])
+    o2 = mx.sym.ones(shape=[3,-1])
+    t = mx.sym.transpose(o2)
+    b = o1 + t
+    x = b.bind(mx.cpu(), args={})
+    y = x.forward()
+    assert(y[0].shape == (2,3))
+
+
 if __name__ == '__main__':
     import nose
     nose.runmodule()
diff --git a/tests/python/unittest/test_profiler.py b/tests/python/unittest/test_profiler.py
index b9447f951..d04f390f2 100644
--- a/tests/python/unittest/test_profiler.py
+++ b/tests/python/unittest/test_profiler.py
@@ -267,21 +267,23 @@ def test_aggregate_stats_sorting():
     test_profile_event(False)
     for sb in sort_by_options:
         for asc in ascending_options:
-            debug_str = profiler.dumps(format = 'json', sort_by = sb, ascending = asc)
+            debug_str = profiler.dumps(format='json', sort_by=sb, ascending=asc)
             check_sorting(debug_str, sb, asc)
     profiler.set_state('stop')
 
 def test_aggregate_duplication():
     file_name = 'test_aggregate_duplication.json'
-    enable_profiler(profile_filename = file_name, run=True, continuous_dump=True, \
+    enable_profiler(profile_filename=file_name, run=True, continuous_dump=True, \
                     aggregate_stats=True)
+    # clear aggregate stats
+    profiler.dumps(reset=True)
     inp = mx.nd.zeros(shape=(100, 100))
     y = mx.nd.sqrt(inp)
     inp = inp + 1
     inp = inp + 1
     mx.nd.waitall()
     profiler.dump(False)
-    debug_str = profiler.dumps(format = 'json')
+    debug_str = profiler.dumps(format='json')
     target_dict = json.loads(debug_str)
     assert 'Time' in target_dict and 'operator' in target_dict['Time'] \
         and 'sqrt' in target_dict['Time']['operator'] \
@@ -293,7 +295,7 @@ def test_aggregate_duplication():
     assert target_dict['Time']['operator']['_plus_scalar']['Count'] == 2
     profiler.set_state('stop')
     
-def test_custom_operator_profiling(seed = None, file_name = None):
+def test_custom_operator_profiling(seed=None, file_name=None):
     class Sigmoid(mx.operator.CustomOp):
         def forward(self, is_train, req, in_data, out_data, aux):
             x = in_data[0].asnumpy()
@@ -330,8 +332,10 @@ def test_custom_operator_profiling(seed = None, file_name = None):
 
     if file_name is None:
         file_name = 'test_custom_operator_profiling.json'
-    enable_profiler(profile_filename = file_name, run=True, continuous_dump=True,\
+    enable_profiler(profile_filename=file_name, run=True, continuous_dump=True,\
                     aggregate_stats=True)
+    # clear aggregate stats
+    profiler.dumps(reset=True)
     x = mx.nd.array([0, 1, 2, 3])
     x.attach_grad()
     with mx.autograd.record():
@@ -339,7 +343,7 @@ def test_custom_operator_profiling(seed = None, file_name = None):
     y.backward()
     mx.nd.waitall()
     profiler.dump(False)
-    debug_str = profiler.dumps(format = 'json')
+    debug_str = profiler.dumps(format='json')
     target_dict = json.loads(debug_str)
     assert 'Time' in target_dict and 'Custom Operator' in target_dict['Time'] \
         and 'MySigmoid::pure_python' in target_dict['Time']['Custom Operator'] \
@@ -347,8 +351,15 @@ def test_custom_operator_profiling(seed = None, file_name = None):
         and 'MySigmoid::_zeros' in target_dict['Time']['Custom Operator']
     profiler.set_state('stop')
 
-def test_custom_operator_profiling_multiple_custom_ops_imperative(seed = None, \
-        mode = 'imperative', file_name = None):
+def check_custom_operator_profiling_multiple_custom_ops_output(debug_str):
+    target_dict = json.loads(debug_str)
+    assert 'Time' in target_dict and 'Custom Operator' in target_dict['Time'] \
+        and 'MyAdd1::pure_python' in target_dict['Time']['Custom Operator'] \
+        and 'MyAdd2::pure_python' in target_dict['Time']['Custom Operator'] \
+        and 'MyAdd1::_plus_scalar' in target_dict['Time']['Custom Operator'] \
+        and 'MyAdd2::_plus_scalar' in target_dict['Time']['Custom Operator']
+
+def custom_operator_profiling_multiple_custom_ops(seed, mode, file_name):
     class MyAdd(mx.operator.CustomOp):
         def forward(self, is_train, req, in_data, out_data, aux):        
             self.assign(out_data[0], req[0], in_data[0] + 1)
@@ -392,65 +403,47 @@ def test_custom_operator_profiling_multiple_custom_ops_imperative(seed = None, \
         def create_operator(self, ctx, shapes, dtypes):
             return MyAdd()
 
-    if file_name is None:
-        file_name = 'test_custom_operator_profiling_multiple_custom_ops_imperative.json'
-    enable_profiler(profile_filename = file_name, run=True, continuous_dump=True,\
+    enable_profiler(profile_filename=file_name, run=True, continuous_dump=True,\
                     aggregate_stats=True)
+    # clear aggregate stats
+    profiler.dumps(reset=True)
     inp = mx.nd.zeros(shape=(100, 100))
     if mode == 'imperative':
-        x = inp + 1
         y = mx.nd.Custom(inp, op_type='MyAdd1')
         z = mx.nd.Custom(inp, op_type='MyAdd2')
     elif mode == 'symbolic':
         a = mx.symbol.Variable('a')
-        b = a + 1
-        c = mx.symbol.Custom(data=a, op_type='MyAdd1')
-        d = mx.symbol.Custom(data=a, op_type='MyAdd2')
-        b.bind(mx.cpu(), {'a': inp}).forward()
-        c.bind(mx.cpu(), {'a': inp}).forward()
-        d.bind(mx.cpu(), {'a': inp}).forward()
+        b = mx.symbol.Custom(data=a, op_type='MyAdd1')
+        c = mx.symbol.Custom(data=a, op_type='MyAdd2')
+        y = b.bind(mx.cpu(), {'a': inp})
+        z = c.bind(mx.cpu(), {'a': inp})
+        yy = y.forward()
+        zz = z.forward()
     mx.nd.waitall()
     profiler.dump(False)
-    debug_str = profiler.dumps(format = 'json')
-    target_dict = json.loads(debug_str)
-    '''
-    We are calling _plus_scalar within MyAdd1 and MyAdd2 and outside both the custom 
-    operators, so in aggregate stats we should have three different kinds of 
-    _plus_scalar under domains "Custom Operator" and "operator"
-    '''
-    assert 'Time' in target_dict and 'Custom Operator' in target_dict['Time'] \
-        and 'MyAdd1::pure_python' in target_dict['Time']['Custom Operator'] \
-        and 'MyAdd2::pure_python' in target_dict['Time']['Custom Operator'] \
-        and 'MyAdd1::_plus_scalar' in target_dict['Time']['Custom Operator'] \
-        and 'MyAdd2::_plus_scalar' in target_dict['Time']['Custom Operator'] \
-        and '_plus_scalar' not in target_dict['Time']['Custom Operator'] \
-        and 'operator' in target_dict['Time'] \
-        and '_plus_scalar' in target_dict['Time']['operator']
+    debug_str = profiler.dumps(format='json')
+    check_custom_operator_profiling_multiple_custom_ops_output(debug_str)
     profiler.set_state('stop')
-
-@unittest.skip("Flaky test https://github.com/apache/incubator-mxnet/issues/15406")
+    
 def test_custom_operator_profiling_multiple_custom_ops_symbolic():
-    run_in_spawned_process(test_custom_operator_profiling_multiple_custom_ops_imperative, \
-            {'MXNET_EXEC_BULK_EXEC_INFERENCE' : 0, \
-            'MXNET_EXEC_BULK_EXEC_TRAIN' : 0}, \
-            'symbolic', \
+    custom_operator_profiling_multiple_custom_ops(None, 'symbolic', \
             'test_custom_operator_profiling_multiple_custom_ops_symbolic.json')
 
+def test_custom_operator_profiling_multiple_custom_ops_imperative():
+    custom_operator_profiling_multiple_custom_ops(None, 'imperative', \
+            'test_custom_operator_profiling_multiple_custom_ops_imperative.json')
+
 @unittest.skip("Flaky test https://github.com/apache/incubator-mxnet/issues/15406")
 def test_custom_operator_profiling_naive_engine():
     # run the three tests above using Naive Engine
     run_in_spawned_process(test_custom_operator_profiling, \
             {'MXNET_ENGINE_TYPE' : "NaiveEngine"}, \
             'test_custom_operator_profiling_naive.json')
-    run_in_spawned_process(test_custom_operator_profiling_multiple_custom_ops_imperative, \
-            {'MXNET_ENGINE_TYPE' : "NaiveEngine"}, \
-            'imperative', \
+    run_in_spawned_process(custom_operator_profiling_multiple_custom_ops, \
+            {'MXNET_ENGINE_TYPE' : "NaiveEngine"}, 'imperative', \
             'test_custom_operator_profiling_multiple_custom_ops_imperative_naive.json')
-    run_in_spawned_process(test_custom_operator_profiling_multiple_custom_ops_imperative, \
-            {'MXNET_ENGINE_TYPE' : "NaiveEngine", \
-            'MXNET_EXEC_BULK_EXEC_INFERENCE' : 0, \
-            'MXNET_EXEC_BULK_EXEC_TRAIN' : 0}, \
-            'symbolic', \
+    run_in_spawned_process(custom_operator_profiling_multiple_custom_ops, \
+            {'MXNET_ENGINE_TYPE' : "NaiveEngine"}, 'symbolic', \
             'test_custom_operator_profiling_multiple_custom_ops_symbolic_naive.json')
 
 if __name__ == '__main__':
diff --git a/tests/python/unittest/test_subgraph.py b/tests/python/unittest/test_subgraph.py
index b5577d4d0..4c13f9c70 100644
--- a/tests/python/unittest/test_subgraph.py
+++ b/tests/python/unittest/test_subgraph.py
@@ -144,6 +144,59 @@ def test_make_subgraph():
                         rtol=0.001, atol=0.0001)
 
 
+def test_subgraph_with_customOp():
+    class MyAdd(mx.operator.CustomOp):
+        def forward(self, is_train, req, in_data, out_data, aux):
+            self.assign(out_data[0], req[0], in_data[0] + 1)
+
+        def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
+            self.assign(in_grad[0], req[0], out_grad[0])
+
+    @mx.operator.register('MyAdd1')
+    class MyAdd1Prop(mx.operator.CustomOpProp):
+        def __init__(self):
+            super(MyAdd1Prop, self).__init__(need_top_grad=True)
+
+        def list_arguments(self):
+            return ['data']
+
+        def list_outputs(self):
+            return ['output']
+
+        def infer_shape(self, in_shape):
+            # inputs, outputs, aux
+            return [in_shape[0]], [in_shape[0]], []
+
+        def create_operator(self, ctx, shapes, dtypes):
+            return MyAdd()
+
+    @mx.operator.register('MyAdd2')
+    class MyAdd2Prop(mx.operator.CustomOpProp):
+        def __init__(self):
+            super(MyAdd2Prop, self).__init__(need_top_grad=True)
+
+        def list_arguments(self):
+            return ['data']
+
+        def list_outputs(self):
+            return ['output']
+
+        def infer_shape(self, in_shape):
+            # inputs, outputs, aux
+            return [in_shape[0]], [in_shape[0]], []
+
+        def create_operator(self, ctx, shapes, dtypes):
+            return MyAdd()
+
+    inp = mx.nd.zeros(shape=(100, 100))
+    a = mx.symbol.Variable('a')
+    b = a + 1
+    b = mx.symbol.Custom(data=a, op_type='MyAdd1')
+    c = mx.symbol.Custom(data=a, op_type='MyAdd2')
+    b.bind(mx.cpu(), {'a': inp}).forward()
+    c.bind(mx.cpu(), {'a': inp}).forward()
+    mx.nd.waitall()
+
 if __name__ == '__main__':
     import nose
     nose.runmodule()
diff --git a/tests/python/unittest/test_symbol.py b/tests/python/unittest/test_symbol.py
index 2dfe3e44e..963b32493 100644
--- a/tests/python/unittest/test_symbol.py
+++ b/tests/python/unittest/test_symbol.py
@@ -22,7 +22,7 @@ import mxnet as mx
 import numpy as np
 from common import assertRaises, models
 from mxnet.base import NotImplementedForSymbol
-from mxnet.test_utils import discard_stderr
+from mxnet.test_utils import discard_stderr, rand_shape_nd
 import pickle as pkl
 
 def test_symbol_basic():
@@ -188,6 +188,21 @@ def test_symbol_infer_shape_var():
     assert arg_shapes[1] == overwrite_shape
     assert out_shapes[0] == overwrite_shape
 
+
+def test_symbol_magic_abs():
+    for dim in range(1, 7):
+        with mx.name.NameManager():
+            data = mx.symbol.Variable('data')
+            method = data.abs(name='abs0')
+            magic = abs(data)
+            regular = mx.symbol.abs(data, name='abs0')
+            ctx = {'ctx': mx.context.current_context(), 'data': rand_shape_nd(dim)}
+            mx.test_utils.check_consistency(
+                [method, magic], ctx_list=[ctx, ctx])
+            mx.test_utils.check_consistency(
+                [regular, magic], ctx_list=[ctx, ctx])
+
+
 def test_symbol_fluent():
     has_grad = set(['flatten', 'expand_dims', 'flip', 'tile', 'transpose', 'sum', 'nansum', 'prod',
                     'nanprod', 'mean', 'max', 'min', 'reshape', 'broadcast_to', 'split',
@@ -242,6 +257,7 @@ def test_symbol_fluent():
     check_fluent_regular('reshape', {'shape': (17, 1, 5)})
     check_fluent_regular('broadcast_to', {'shape': (5, 17, 47)})
     check_fluent_regular('squeeze', {'axis': (1, 3)}, shape=(2, 1, 3, 1, 4))
+    check_fluent_regular('squeeze', {}, shape=(2, 1, 3, 1, 4))
 
 def check_symbol_consistency(sym1, sym2, ctx, skip_grad=False, equal_nan=False):
     assert sym1.list_arguments() == sym2.list_arguments()
diff --git a/tests/tutorials/test_tutorials.py b/tests/tutorials/test_tutorials.py
index 0c4954acb..c2173a7dc 100644
--- a/tests/tutorials/test_tutorials.py
+++ b/tests/tutorials/test_tutorials.py
@@ -133,6 +133,9 @@ def test_gluon_learning_rate_schedules_advanced():
 def test_gluon_info_gan():
     assert _test_tutorial_nb('gluon/info_gan')
 
+def test_gluon_fit_api_fashion_mnist():
+    assert _test_tutorial_nb('gluon/fit_api_tutorial')
+
 def test_nlp_cnn():
     assert _test_tutorial_nb('nlp/cnn')
 
@@ -213,3 +216,6 @@ def test_control_flow():
 
 def test_amp():
     assert _test_tutorial_nb('amp/amp_tutorial')
+
+def test_mkldnn_quantization():
+    assert _test_tutorial_nb('mkldnn/mkldnn_quantization')
\ No newline at end of file
